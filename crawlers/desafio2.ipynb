{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafios idwall\n",
    "\n",
    "\n",
    "## Resolu√ß√£o da Parte 1 do Desafio 2 - Crawlers\n",
    "\n",
    "Web Scraping do Reddit, escrito em Python, utilizando as bibliotecas _request_ e _BeautifulSoup_.\n",
    "\n",
    "#### Desafio:\n",
    "Encontrar e listar as _threads_ com 5000 pontos ou mais no Reddit naquele momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolu√ß√£o:\n",
    "\n",
    "√â poss√≠vel realizar o scrapping via PRAW, um wrapper para o API do Reddit, o qual permite realizar scrapings dos subreddits, criar um bot, entre outras funcionalidades.\n",
    "\n",
    "Seguem dois sites que ensinam como realizar scrapping por esse m√©todo:\n",
    "\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "\n",
    "Mas aqui, com o intuito de demonstrar habilidades mais gerais, vamos realizar o scrapping utilizando os pacotes 'request' e 'BeautifulSoup'.\n",
    "\n",
    "Para uma breve introdu√ß√£o sobre web scraping e aplica√ß√£o destes pacotes ver:\n",
    "\n",
    "https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k&t=1428s\n",
    "\n",
    "\n",
    "Dito isto, vamos come√ßar pelo simples e buscar as _top threads_ dentro do subreddit 'r/AskReddit': https://www.reddit.com/r/AskReddit/top/?t=day\n",
    "\n",
    "\"Subreddits s√£o como f√≥runs dentro do Reddit e as postagens s√£o chamadas threads.\n",
    "\n",
    "Para quem gosta de gatos, h√° o subreddit '/r/cats' com threads contendo fotos de gatos fofinhos. Para threads sobre o Brasil, vale a pena visitar '/r/brazil' ou ainda '/r/worldnews'. Um dos maiores subreddits √© o '/r/AskReddit'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a url e salvando o arquivo em html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importam-se as bibliotecas necess√°rias para o scrapping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# E cria-se uma fun√ß√£o para salvar e outra para\n",
    "# abrir a p√°gina html, a fim de minimizar danos ao servidor:\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "# Eu deixei essas fun√ß√µes salvas num arquivo chamado\n",
    "# save_open_html.py para consultas futuras, se necess√°rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATEN√á√ÉO!!!\n",
    "\n",
    "Recomenda-se n√£o rodar o c√≥digo da c√©lula abaixo, sendo utilizado assim apenas para simples confer√™ncia.\n",
    "\n",
    "Como o reddit possui um sistema automatizado que impede mais que um request a cada dois segundos √© poss√≠vel que o c√≥digo abaixo gere um \"erro\", de forma a n√£o ser capaz de obter o c√≥digo html do site. Eu tentei procurar entender o porqu√™, mas n√£o obtive uma resposta.\n",
    "\n",
    "Mas caso queira rodar o c√≥digo, √© necess√°rio tentar algumas vezes, caso n√£o consiga de primeira, at√© conseguir obter o html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Por seguran√ßa, esta c√©lula est√° com formata√ß√£o Raw NBConvert ###\n",
    "\n",
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/r/AskReddit/top/?t=day'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'askreddit_top_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   var __SUPPORTS_TIMING_API = typeof performance === 'object' && !!performance.mark && !! performance.measure && !!performance.getEntriesByType;\n",
      "          function __perfMark(name) { __SUPPORTS_TIMING_API && performance.mark(name); };\n",
      "          var __firstLoaded = false;\n",
      "          function __markFirstPostVisible() {\n",
      "            if (__firstLoaded) { return; }\n",
      "            __firstLoaded = true;\n",
      "            __perfMark(\"first_post_title_image_loaded\");\n",
      "          }\n",
      "  </script>\n",
      "  <script>\n",
      "   __perfMark('head_tag_start');\n",
      "  </script>\n",
      "  <title>\n",
      "   Ask Reddit...\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <style>\n",
      "   /* http://meyerweb.com/eric/tools/css/reset/\n",
      "    v2.0 | 20110126\n",
      "    License: none (public domain)\n",
      "  */\n",
      "\n",
      "  html, body, div, span, applet, object, iframe,\n",
      "  h1, h2, h3, h4, h5, h6, p, blockquote, pre,\n",
      "  a, \n"
     ]
    }
   ],
   "source": [
    "# Para abrir localmente e trabalhar com o arquivo e n√£o com v√°rios requests:\n",
    "html = open_html('askreddit_top_day')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pode-se tamb√©m abrir a p√°gina desejada direto do servidor fonte em formato html:\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "### Vale lembrar! ###\n",
    "# Esse m√©todo, pode ser danoso aos donos do site a ser requisitado, ou em alguns casos n√£o funcionar. Um script em python, se codificado incorretamente, pode executar milhares de requests por segundo, possivelmente tirando o site do ar, e podendo causar danos financeiros aos donos do site em quest√£o. Por conta disso, muitos sites impedem requisi√ß√µes consecutivas como medida de seguran√ßa."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Exemplos b√°sicos, apenas como \"anota√ß√£o\" para consulta r√°pida ###\n",
    "\n",
    "## Outra forma para importar uma p√°gina desejada salva num arquivo:\n",
    "\n",
    "with open('simple.html') as html_file:\n",
    "     soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "\n",
    "## Exemplos de m√©todos BeautifulSoup:\n",
    "\n",
    "# Mostra o primeiro t√≠tulo da pagina em formato de texto:\n",
    "match1 = soup.title.text\n",
    "\n",
    "# Mostra o primeiro div da pagina contendo classe, headers e paragrafos:\n",
    "match2 = soup.div\n",
    "\n",
    "# M√©todo .find('tag') que retorna a primeira correpond√™ncia as tags podem ser 'div', 'a', 'h3', etc., e a classe pode ser 'footer', 'article', etc.:\n",
    "match3 = soup.find('div', class_='article')\n",
    "\n",
    "\n",
    "## Caso seja necess√°rio, √© poss√≠vel utilizar o m√©todo try do python para testar e retornar valores, caso n√£o se encontre um desejado resultado:\n",
    "\n",
    "try:\n",
    "    (C√ìDIGO DE SCRAPPING)\n",
    "    # pass\n",
    "except Exception as e:\n",
    "    pass # Ou uma atribui√ß√£o. Exemplo seria atribuir None a algum par√¢metro \"raspado\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o parsing no c√≥digo reddit:\n",
    "\n",
    "Primeiramente vamos analisar se o primeiro top thread √© o desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## html do bloco completo contendo todos os coment√°rios\n",
    "all_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "## html da tag e classe do n√∫mero de pontos de certa thread:\n",
    "# <div class=\"_1rZYMD_4xY3gRcSS3p8ODO\" style=\"color:#1A1A1B\">22.8k</div>\n",
    "pontos_thread = all_threads.find('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "pontos_primeira_thread = pontos_thread.text\n",
    "\n",
    "## html da tag e classe do texto de uma certa thread\n",
    "# <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3>\n",
    "thread = all_threads.find('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "texto_thread = thread.text\n",
    "\n",
    "## html do link do texto\n",
    "# <a data-click-id=\"body\" class=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\" href=\"/r/AskReddit/comments/cyqtk2/teachers_of_reddit_what_was_the_most_obvious/\"><div class=\"_2SdHzo12ISmrC8H86TgSCp _3wqmjmv3tb_k-PROt7qFZe \" style=\"--posttitletextcolor:#444e59\" theme=\"[object Object]\"><h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3></div></a>\n",
    "link = all_threads.find('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "referencia = link['href']\n",
    "texto_link = f'https://www.reddit.com{referencia}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8k\n",
      "Everyone has a scar on their body from something dumb, they did as a child. What's your story?\n",
      "https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/\n"
     ]
    }
   ],
   "source": [
    "# Resultado dos pontos e texto do primeiro top thread do dia:\n",
    "print(pontos_primeira_thread)\n",
    "print(texto_thread)\n",
    "print(texto_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo certo at√© aqui, ent√£o vamos seguir com os pr√≥ximos passos.\n",
    "\n",
    "### Realizando um loop sobre todas as top threads desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "        \n",
    "print(len(extracted_points))\n",
    "print(len(extracted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse c√≥digo, √© possivel ver que a lista contendo as pontua√ß√µes est√° duplicada. Provavelmente isto ocorre por haverem classes repitidas no c√≥digo html. Para consertar isto, basta criarmos uma nova lista contendo os valores √∫nicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28.8k', '28.8k', '3.3k', '3.3k', '2.7k', '2.7k', '1.2k', '1.2k', '1.5k', '1.5k', '812', '812', '936', '936', '563', '563']\n",
      "['28.8k', '3.3k', '2.7k', '1.2k', '1.5k', '812', '936', '563']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_points)\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "    \n",
    "print(unique_extracted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': \"Everyone has a scar on their body from something dumb, they did as a child. What's your story?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/'}]\n"
     ]
    }
   ],
   "source": [
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            \n",
    "print(top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opcionalmente podemos salvar o resultando em um arquivo json ou csv para uso futuro\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('data.json', 'w') as outfile:\n",
    "#     json.dump(top_threads, outfile, indent=4)\n",
    "    \n",
    "import csv\n",
    "csv_file = open('data.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['pontuacao', 'subreddit', 'thread', 'link'])\n",
    "\n",
    "# inserir csv_writer.writerow([p, s , t , l]) no loop das \"top_threads = []\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping na pagina principal\n",
    "\n",
    "Agora vamos ir para uma p√°gina mais geral do site www.reddit.com, e realizar um novo scrape (Novamente, a c√©lula abaixo foi convertida em Raw NBConvert, para evitar rod√°-la acidentalmente):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/top/'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'reddit_top_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pontuacao': '121k', 'subreddit': '/r/aww', 'titulo thread': 'Scared cat gets saved by two French guys', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyv97r/scared_cat_gets_saved_by_two_french_guys/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/memes', 'titulo thread': 'The Area 51 raid is still happening right?', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cz2i20/the_area_51_raid_is_still_happening_right/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/pics', 'titulo thread': 'In 1964, Ringo Starr snapped a photo of some high school students who skipped class to see the Beatles during their first trip to the US. The group had no idea the photo existed until Ringo published his book of photos. Nearly 50 years later, the group reunited and recreated the photo.', 'link para os comentarios': 'https://www.reddit.com/r/pics/comments/cyx1os/in_1964_ringo_starr_snapped_a_photo_of_some_high/'}\n",
      "\n",
      "{'pontuacao': '103k', 'subreddit': '/r/aww', 'titulo thread': 'Found a tiny danger noodle in need of assistance üêç', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyx8xb/found_a_tiny_danger_noodle_in_need_of_assistance/'}\n",
      "\n",
      "{'pontuacao': '99.8k', 'subreddit': '/r/memes', 'titulo thread': 'I can feel it', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cyu0z3/i_can_feel_it/'}\n",
      "\n",
      "{'pontuacao': '84.7k', 'subreddit': '/r/MurderedByWords', 'titulo thread': \"It's never just about a cake\", 'link para os comentarios': 'https://www.reddit.com/r/MurderedByWords/comments/cyt56i/its_never_just_about_a_cake/'}\n",
      "\n",
      "{'pontuacao': '80.1k', 'subreddit': '/r/mildlyinteresting', 'titulo thread': 'This vine climbed up a chair to silence my wind chime.', 'link para os comentarios': 'https://www.reddit.com/r/mildlyinteresting/comments/cyw9xx/this_vine_climbed_up_a_chair_to_silence_my_wind/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C√≥digo restante:\n",
    "\n",
    "html = open_html('reddit_top_today')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "\n",
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            csv_writer.writerow([p, s , t , l])\n",
    "\n",
    "csv_file.close()\n",
    "for i in range(len(top_threads)):\n",
    "    print(top_threads[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping em uma lista de subreddits\n",
    "\n",
    "Agora que entendemos como realizar o scrappping em um √∫nico link, o pr√≥ximo passo seria realizar o scraping a partir de uma lista de subreddits separados por ponto-e-v√≠rgula, e.g., \"programming;dogs;brazil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "askreddit;worldnews;cats\n",
      "\n",
      " ['askreddit', 'worldnews', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# input: askreddit;worldnews;cats\n",
    "subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "subreddits_separated = subreddits.split(';')\n",
    "\n",
    "print('\\n', subreddits_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/askreddit/top/?t=day', 'https://www.reddit.com/r/worldnews/top/?t=day', 'https://www.reddit.com/r/cats/top/?t=day']\n"
     ]
    }
   ],
   "source": [
    "# A partir da lista criada anteriormente, cria-se outra lista com as urls para download do c√≥digo html\n",
    "urls_subreddits = []\n",
    "for i in range(len(subreddits_separated)):\n",
    "    url_link = 'https://www.reddit.com/r/{}/top/?t=day'.format(subreddits_separated[i])\n",
    "    urls_subreddits.append(url_link)\n",
    "\n",
    "print(urls_subreddits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Aqui vamos salvar os htmls usando um for loop:\n",
    "\n",
    "# for i in range(0, len(urls_subreddits)):\n",
    "for i in range(0, 1):\n",
    "    url = urls_subreddits[i]\n",
    "    codigo_html = requests.get(url)\n",
    "    save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indo para a pasta principal, √© poss√≠vel ver que os arquivos foram salvos. Mas tem casos em que rodando o c√≥digo acima, por conta da prote√ß√£o que o site reddit possui, os htmls salvos n√£o correspondem aos desejados. Nesses casos √© necess√°ria uma interven√ß√£o humana no looping para ser poss√≠vel obter os htmls desejados, tendo que baixar um por vez.\n",
    "\n",
    "A partir da lista de htmls √© poss√≠vel realizar o mesmo scrapping anterior utilizando um loop, como mostrado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/askreddit:  [{'pontuacao': '33.4k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What do people THINK is a scam, but they actually just don't understand it?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0dlvo/what_do_people_think_is_a_scam_but_they_actually/'}, {'pontuacao': '30.9k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What's a negative fact about marijuana we should know?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0aoi5/whats_a_negative_fact_about_marijuana_we_should/'}, {'pontuacao': '40.2k', 'subreddit': '/r/AskReddit', 'titulo thread': \"The 2010's decade will be over in 4 months. What do you think people will remember this decade for?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0jjc2/the_2010s_decade_will_be_over_in_4_months_what_do/'}, {'pontuacao': '24.5k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What's the worst case you've seen of someone trying to play the victim when it was all their own fault?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0fwvx/whats_the_worst_case_youve_seen_of_someone_trying/'}, {'pontuacao': '9.7k', 'subreddit': '/r/AskReddit', 'titulo thread': 'What everyday thing seriously creeps you out?', 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d08w7m/what_everyday_thing_seriously_creeps_you_out/'}]\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I‚Äôm so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    # Nesses dois casos (extracted_points e extracted_texts), √© poss√≠vel criarmos uma fun√ß√£o, \n",
    "    # mas aqui como achei que o c√≥digo n√£o possui uma recorr√™ncia alta desses loops\n",
    "    # preferi deix√°-los expl√≠citos\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread) \n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "    \n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "            likes = p[0:-1]\n",
    "            likes = int(float(likes)*1000)\n",
    "            if likes >= 5000:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tamb√©m modificar o limite de pontos para um m√≠nimo de 500 pontos por exemplo. Como mostrado abaixo:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread)\n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "\n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "    \n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1: # Note que a partir daqui, o c√≥digo foi modificado com o fim de obter as threads com pontua√ß√£o menor que 1k\n",
    "            likes = p\n",
    "            if p[-1] == 'k':\n",
    "                likes = p[0:-1]\n",
    "                likes = int(float(likes)*1000)\n",
    "            else:\n",
    "                likes = int(likes)\n",
    "            if likes >= 500:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "    if top_threads == []:\n",
    "        top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print(unique_extracted_points)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima, juntamente com os threads, tamb√©m mandei imprimir os valores dos likes para compararmos os resultados e tentar buscar por erros. Se rodarmos o c√≥digo, aparentemente estar√° tudo certo.\n",
    "\n",
    "Nos casos em que n√£o temos threads com o valor de pontos desejados, nosso c√≥digo n√£o retornaria resultados para esses casos, retornando ao inv√©s disso a resposta \"Nao ha top threads com os requisitos [...]\".\n",
    "\n",
    "## Resolu√ß√£o \"extra\" - Fun√ß√£o para 'reddit scrapping':\n",
    "Por fim, podemos criar uma fun√ß√£o que recebe os subreddits desejados e retorna as top threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from save_open_html import *\n",
    "\n",
    "def top_threads(subreddits):\n",
    "#     import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "#     subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "    subreddits_separated = subreddits.split(';')\n",
    "    \n",
    "    urls_subreddits = []\n",
    "    for i in range(len(subreddits_separated)):\n",
    "        url_link = 'https://www.reddit.com/r/{}/top/?t=day'.format(subreddits_separated[i]) \n",
    "        urls_subreddits.append(url_link)\n",
    "    \n",
    "#     Aqui salvar√≠amos os htmls usando um for loop:\n",
    "#\n",
    "#     for i in range(0, len(urls_subreddits)):\n",
    "#         url = urls_subreddits[i]\n",
    "#         codigo_html = requests.get(url)\n",
    "#         save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "    for i in subreddits_separated:\n",
    "\n",
    "        html = open_html(f'reddit_{i}')\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "        all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "        all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "        all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "        extracted_points = []\n",
    "        for points in all_points_thread:\n",
    "            point = points.text\n",
    "            extracted_points.append(point)\n",
    "\n",
    "        extracted_texts = []\n",
    "        for threads in all_texts:\n",
    "            thread = threads.text\n",
    "            extracted_texts.append(thread)\n",
    "\n",
    "        extracted_links = []\n",
    "        for links in all_links:\n",
    "            referencia = links['href']\n",
    "            if referencia.startswith('http'):\n",
    "                extracted_links.append(referencia)\n",
    "            else:\n",
    "                texto_link = f'https://www.reddit.com{referencia}'\n",
    "                extracted_links.append(texto_link)\n",
    "\n",
    "        unique_extracted_points = []\n",
    "        for j in range(0, len(extracted_points), 2):\n",
    "            unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "        top_threads = []\n",
    "        for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "            if len(p) > 1: # Note que a partir daqui, o c√≥digo foi modificado com o fim de obter as threads com pontua√ß√£o menor que 1k\n",
    "                likes = p\n",
    "                if p[-1] == 'k':\n",
    "                    likes = p[0:-1]\n",
    "                    likes = int(float(likes)*1000)\n",
    "                else:\n",
    "                    likes = int(likes)\n",
    "                if likes >= 5000:\n",
    "                    subreddit = l.split('/')[4]\n",
    "                    s = f'/r/{subreddit}'\n",
    "                    record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                    top_threads.append(record)\n",
    "        if top_threads == []:\n",
    "            top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "        print()\n",
    "        print(f'Top threads de /r/{i}: ', top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top threads de /r/programming:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I‚Äôm so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n"
     ]
    }
   ],
   "source": [
    "# inputs pos√≠veis: programming;dogs;brazil;askreddit;worldnews;cats\n",
    "top_threads('programming;cats;worldnews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolu√ß√£o - Parte 2: Bot para Telegram\n",
    "\n",
    "### Criando um bot\n",
    "\n",
    "A seguir, preferi colocar um c√≥digo de bot (um pouco) mais completo com o intuito de reutilizar esse c√≥digo futuramente (seja para revisar alguns conceitos, quanto para poder implementar os comandos ap√≥s as hashtags - afinal, tamb√©m me divirto trabalhando hehe)\n",
    "\n",
    "A base do c√≥digo foi retirada do 'Curso de Python na pr√°tica', presente no youtube, e gravado pelo usu√°rio 11Wills11:\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLsMpSZTgkF5C_Kkc0XBtM3OVLBsjUkjzy\n",
    "\n",
    "Esse bot √© capaz de responder frases simples, tais como 'oi', 'tchau'. Tamb√©m √© poss√≠vel ensinar novas frases utilizando o comando 'aprende'. Ele tamb√©m √© \"customizado\" para tratar diferentemente quem ele \"conhece\" e desconhecidos.\n",
    "\n",
    "Quem ele n√£o conhece, ele retorna \"Muito Prazer (nome)!\", seguido de uma pergunta sobre os threads. E quem ele conhece, retorna \"oi (nome)! te adoro, sua maravilhosa!\", seguido, tamb√©m, de uma pergunta sobre os threads.\n",
    "\n",
    "Ap√≥s o c√≥digo, existem algumas demonstra√ß√µes.\n",
    "\n",
    "Segue o bot com algumas modifica√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um bot b√°sico:\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# import subprocess as sub\n",
    "from save_open_html import *\n",
    "\n",
    "class Chatbot():\n",
    "    # Na inicializa√ß√£o '__init__(self,nome)' desta classe, a lista de conhecidos\n",
    "    # e um hist√≥rico da conversa atual s√£o inicializados junto do\n",
    "    # nome do Chatbot, sendo \"memorizado\" para consultas futuras.\n",
    "    def __init__(self, nome):\n",
    "#         Esses comandos ap√≥s as hashtags, s√£o comandos para implementar um aprendizado de nomes\n",
    "#\n",
    "#         try:\n",
    "#             memoria = open(nome+'.json', 'r')\n",
    "#         except FileNotFoundError:\n",
    "#             memoria = open(nome+'.json', 'w')\n",
    "#             memoria.write('[\"Criadora\", \"Lais\"], {\"oi\": \"oi! qual o seu nome?\", \"tchau\": \"tchau!\", \"nenhum\": \"ok... sinto muito :(\"}')\n",
    "#             memoria.close()\n",
    "#             memoria = open(nome+'.json', 'r')\n",
    "        self.nome = nome\n",
    "        self.conhecidos = ['Lais', 'Renata', 'Mariana']\n",
    "#         self.conhecidos, self.frases = json.load(memoria)\n",
    "#         memoria.close()\n",
    "        self.historico = [None]\n",
    "        self.frases = {'oi': 'oi! qual o seu nome?', 'tchau': 'tchau!', 'nenhum': 'ok... sinto muito :('}\n",
    "    \n",
    "    def escuta(self, frase=None):\n",
    "        if frase == None:\n",
    "            frase = input('>:')\n",
    "        frase = str(frase)\n",
    "        if 'executa ' in frase:\n",
    "            return frase\n",
    "        frase = frase.lower()\n",
    "        return frase\n",
    "    \n",
    "    def pensa(self, frase):\n",
    "        if frase in self.frases:\n",
    "            return self.frases[frase]\n",
    "        \n",
    "        if frase == 'aprende':\n",
    "#             chave = input('Digite a frase: ')\n",
    "#             resp = input('Digite a resposta: ')\n",
    "#             self.frases[chave] = resp\n",
    "#             return 'Aprendido'\n",
    "\n",
    "#       No caso do telegram, temos que substituir os comandos acima para que o bot funcione\n",
    "            return 'Digite a frase: '\n",
    "        \n",
    "#       Os comandos abaixo servem para responder frases que dependem do historico\n",
    "        ultimaFrase = self.historico[-1]\n",
    "        if ultimaFrase == 'oi! qual o seu nome?':\n",
    "            nome = self.pega_nome(frase)\n",
    "            resp = self.responde_nome(nome)\n",
    "            return resp\n",
    "        if ultimaFrase == 'Digite a frase: ':\n",
    "            self.chave = frase\n",
    "            return 'Digite a resposta: '\n",
    "        if ultimaFrase == 'Digite a resposta: ':\n",
    "            resp = frase\n",
    "            self.frases[self.chave] = resp\n",
    "#             gravaMemoria()\n",
    "            return 'Aprendido'\n",
    "\n",
    "        if '/nadaprafazer ' in frase:\n",
    "            subreddit = frase.replace('/nadaprafazer ', '')\n",
    "            try:\n",
    "                return top_threads(subreddit)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            resp = srt(eval(frase))\n",
    "            return resp\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return 'nao te entendi o.o'\n",
    "    \n",
    "    def pega_nome(self, nome):\n",
    "        nome = nome.split(' ')\n",
    "        if len(nome) == 1:\n",
    "            nome = nome[0]\n",
    "        else:\n",
    "            nome = nome[-1]\n",
    "        nome = nome.title()\n",
    "        return nome\n",
    "    \n",
    "    def responde_nome(self, nome):\n",
    "        if nome in self.conhecidos:\n",
    "            frase = 'oi '\n",
    "#             print(frase+nome+'! te adoro, sua maravilhosa!')\n",
    "        else:\n",
    "            frase = 'Muito prazer '\n",
    "        return frase+nome+'! '\n",
    "#             Estes c√≥digos seriam para criar um arquivo contendo todas as pessoas que o bot j√° conversou, para ser implementado junto dos comandos comentados acima (em __init__)\n",
    "#\n",
    "#             self.conhecidos.append(nome)\n",
    "#             memoria = open(self.nome+'.json', 'w')\n",
    "#             json.dump(self.conhecidos, memoria)\n",
    "#             memoria.close()\n",
    "#       ou\n",
    "#             self.gravaMemoria()\n",
    "            \n",
    "#         return 'O que gostaria gostaria de acompanhar hoje?' \n",
    "        \n",
    "    def fala(self, frase):\n",
    "#        Esses comandos abaixo ap√≥s as hashtags seriam para implementar outra funcionalidade no bot: a de abrir documentos, programas e paginas de internet\n",
    "#        No caso do linux, podemos escrever para o bot \"executa chromium\", por exemplo, para que ele abra o navegador.\n",
    "#        Ou por exemplo \"executa https://www.google.com.br\", para abrir o site\n",
    "#\n",
    "#         if 'executa ' in frase:\n",
    "#             plataforma = sys.plataform\n",
    "#             comando = frase.replace('executa ', '')\n",
    "#             if 'win' in plataforma:\n",
    "#                 os.startfile(comando)\n",
    "#             if 'linux' in plataforma:\n",
    "#                 try:\n",
    "#                     sub.Popen(comando)\n",
    "#                 except FileNotFoundError:\n",
    "#                     sub.Popen(['xdg-open', comando])\n",
    "\n",
    "#         else:\n",
    "        print(frase)\n",
    "        self.historico.append(frase)\n",
    "        \n",
    "#         def gravaMemoria(self):\n",
    "#             memoria = open(self.nome+'.json', 'w')\n",
    "#             json.dump([self.conhecidos, self.frases], memoria)\n",
    "#             memoria.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bot funciona com a l√≥gica: Escuta --> Pensa --> Fala.\n",
    "\n",
    "Dentro da Classe 'Chatbot', temos essas fun√ß√µes que o caracterizam e mais outras duas (pega_nome, responde_nome) com o intuito de otimizar a leitura do c√≥digo.\n",
    "\n",
    "Para que o bot fique completo, precisamos de um loop que o mantenha \"ativo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:Lais, sua criadora\n",
      "Muito prazer Criadora!\n",
      "Quais threads gostaria de acompanhar hoje?\n",
      ">:nenhum\n",
      "ok... sinto muito :(\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "#### APENAS EXEMPLO! EVITAR RODAR A CELULA ####\n",
    "Bot = Chatbot('Red - O scrappista')\n",
    "\n",
    "while True:\n",
    "    frase = Bot.escuta()\n",
    "    resp = Bot.pensa(frase)\n",
    "    Bot.fala(resp)\n",
    "    if resp == 'tchau!':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assim, podemos criar uma fun√ß√£o que executa o bot em quest√£o:\n",
    "\n",
    "def executa_bot(nome_do_bot):\n",
    "    Bot = Chatbot(nome_do_bot)\n",
    "    while True:\n",
    "        frase = Bot.escuta()\n",
    "        resp = Bot.pensa(frase)\n",
    "        Bot.fala(resp)\n",
    "        if resp == 'tchau!':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:Mariana\n",
      "oi Mariana! te adoro, sua maravilhosa!\n",
      "Quais threads gostaria de acompanhar hoje?\n",
      ">:nenhum\n",
      "ok... sinto muito :(\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "### APENAS EXEMPLO! EVITAR RODAR A CELULA ###\n",
    "executa_bot('Red - O scrappista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:La√≠s\n",
      "Muito prazer La√≠s! \n",
      ">:/NadaPraFazer cats\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I‚Äôm so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "None\n",
      ">:/NadaPraFazer dogs;worldnews\n",
      "\n",
      "Top threads de /r/dogs:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n",
      "None\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "# Utilize este para \"brincar\", ou testar, o c√≥digo,\n",
    "# lembrando que o bot ainda n√£o sabe retornar as top threads :)\n",
    "\n",
    "executa_bot('Red - O scrappista')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferindo o bot para telegram\n",
    "\n",
    "Primeiramente, precisamos criar um bot no servidor do telegram. Pra isso √© necess√°rio ter uma conta e logar no telegram. Na p√°gina do telegram, na caixa de busca, escreve-se \"botfather\". Clicando no primeiro perfil, que √© um bot do telegram para criar bots, vai aparecer algumas op√ß√µes, dentre elas a \"/newbot\". Seguindo os passos, √© poss√≠vel criar o bot inicial.\n",
    "\n",
    "No meu caso, o nome escolhido foi 'RedScrappista', e o token de acesso √© o:\n",
    "\n",
    "989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\n",
    "\n",
    "Vamos ent√£o importar o m√≥dulo 'telepot' e criar o bot com o token dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7ee77980169f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Para pegar as mensagens que o bot recebeu (nesse caso eu enviei um 'oi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# antes de rodar essa c√©lula:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUpdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/__init__.py\u001b[0m in \u001b[0;36mgetUpdates\u001b[0;34m(self, offset, limit, timeout, allowed_updates)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;34m\"\"\" See: https://core.telegram.org/bots/api#getupdates \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'getUpdates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rectify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m     def setWebhook(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/__init__.py\u001b[0m in \u001b[0;36m_api_request\u001b[0;34m(self, method, params, files, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request_with_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(req, **user_kw)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# `fn` must be thread-safe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "bot = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "\n",
    "# Para pegar as mensagens que o bot recebeu (nesse caso eu enviei um 'oi'\n",
    "# antes de rodar essa c√©lula:\n",
    "bot.getUpdates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_id': 11,\n",
       " 'from': {'id': 989962435,\n",
       "  'is_bot': True,\n",
       "  'first_name': 'RedScrappista',\n",
       "  'username': 'RedScrappistaBot'},\n",
       " 'chat': {'id': 937485481,\n",
       "  'first_name': 'La√≠s',\n",
       "  'last_name': 'Alves',\n",
       "  'type': 'private'},\n",
       " 'date': 1567816816,\n",
       " 'text': 'ol√° lais'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para enviar mensagens, o primeiro argumento do m√©todo abaixo √© o 'chat' 'id':\n",
    "bot.sendMessage(937485481, 'ol√° lais')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso vc tiver o telegram aberto, √© poss√≠vel gerar e verificar essas intera√ß√µes (desde que vc modifique o chat id) abrindo o bot no telegram atrav√©s do link:\n",
    "\n",
    "t.me/RedScrappistaBot\n",
    "\n",
    "√â poss√≠vel tamb√©m receber mensagens de uma forma mais f√°cil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ol√°, agora sao 8:05pm\n",
      "e agora se passaram 20 segundos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b04725561c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "bot = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "\n",
    "def recebendoMsg(msg):\n",
    "    print(msg['text'])\n",
    "\n",
    "bot.message_loop(recebendoMsg)\n",
    "\n",
    "while True:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabendo como se envia e se recebe mensagens atrav√©s dos m√©todos de bot do m√≥dulo \"telepot\", vamos ent√£o integrar o c√≥digo do chatbot escrito anteriormente, com a interface do telegram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e3465c8260fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "# from Chatbot import Chatbot\n",
    "telegram = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "bot = Chatbot(\"RedScrappista\")\n",
    "\n",
    "def recebendoMsg(msg):\n",
    "    frase = bot.escuta(frase=msg['text'])\n",
    "    resp = bot.pensa(frase)\n",
    "    bot.fala(resp)\n",
    "    chatID = msg['chat']['id']\n",
    "    # Esse m√©todo abaixo √© correspondente ao de cima\n",
    "    # tipoMsg, tipoChat, chatID = telepot.glance(msg) \n",
    "    telegram.sendMessage(chatID, resp)\n",
    "\n",
    "telegram.message_loop(recebendoMsg)\n",
    "\n",
    "while True:\n",
    "    pass\n",
    "# O loop acima √© um loop infinito para ser executado durante a conversa sem parar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma √© poss√≠vel enviar e receber mensagens para o bot atrav√©s do telegram.\n",
    "\n",
    "O √∫nico problema √© que n√£o consegui configurar o comando /NadaPraFazer, dentro do telegram. E quando envio o comando, aqui na minha m√°quina ele demora para ser processado.\n",
    "\n",
    "(Gostaria de obter ajuda dos universit√°rios nessa rs fiquei bem curiosa como fazer ^^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
