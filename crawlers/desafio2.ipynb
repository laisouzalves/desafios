{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafios idwall\n",
    "\n",
    "\n",
    "## Resolução da Parte 1 do Desafio 2 - Crawlers\n",
    "\n",
    "Web Scraping do Reddit, escrito em Python, utilizando as bibliotecas _request_ e _BeautifulSoup_.\n",
    "\n",
    "#### Desafio:\n",
    "Encontrar e listar as _threads_ com 5000 pontos ou mais no Reddit naquele momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução:\n",
    "\n",
    "É possível realizar o scrapping via PRAW, um wrapper para o API do Reddit, o qual permite realizar scrapings dos subreddits, criar um bot, entre outras funcionalidades.\n",
    "\n",
    "Seguem dois sites que ensinam como realizar scrapping por esse método:\n",
    "\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "\n",
    "Mas aqui, com o intuito de demonstrar habilidades mais gerais, vamos realizar o scrapping utilizando os pacotes 'request' e 'BeautifulSoup'.\n",
    "\n",
    "Para uma breve introdução sobre web scraping e aplicação destes pacotes ver:\n",
    "\n",
    "https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k&t=1428s\n",
    "\n",
    "\n",
    "Dito isto, vamos começar pelo simples e buscar as _top threads_ dentro do subreddit 'r/AskReddit': https://www.reddit.com/r/AskReddit/top/?t=day\n",
    "\n",
    "\"Subreddits são como fóruns dentro do Reddit e as postagens são chamadas threads.\n",
    "\n",
    "Para quem gosta de gatos, há o subreddit '/r/cats' com threads contendo fotos de gatos fofinhos. Para threads sobre o Brasil, vale a pena visitar '/r/brazil' ou ainda '/r/worldnews'. Um dos maiores subreddits é o '/r/AskReddit'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a url e salvando o arquivo em html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importam-se as bibliotecas necessárias para o scrapping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# E cria-se uma função para salvar e outra para\n",
    "# abrir a página html, a fim de minimizar danos ao servidor:\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "    \n",
    "# Eu deixei essas funções salvas num arquivo chamado\n",
    "# save_open_html.py para consultas futuras, se necessário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATENÇÃO!!!\n",
    "\n",
    "Recomenda-se não rodar o código da célula abaixo, sendo utilizado assim apenas para simples conferência.\n",
    "\n",
    "Como o reddit possui um sistema automatizado que impede mais que um request a cada dois segundos é possível que o código abaixo gere um \"erro\", de forma a não ser capaz de obter o código html do site. Eu tentei procurar entender o porquê, mas não obtive uma resposta.\n",
    "\n",
    "Mas caso queira rodar o código, é necessário tentar algumas vezes, caso não consiga de primeira, até conseguir obter o html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Por segurança, esta célula está com formatação Raw NBConvert ###\n",
    "\n",
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/r/AskReddit/top/?t=day'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'askreddit_top_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   var __SUPPORTS_TIMING_API = typeof performance === 'object' && !!performance.mark && !! performance.measure && !!performance.getEntriesByType;\n",
      "          function __perfMark(name) { __SUPPORTS_TIMING_API && performance.mark(name); };\n",
      "          var __firstLoaded = false;\n",
      "          function __markFirstPostVisible() {\n",
      "            if (__firstLoaded) { return; }\n",
      "            __firstLoaded = true;\n",
      "            __perfMark(\"first_post_title_image_loaded\");\n",
      "          }\n",
      "  </script>\n",
      "  <script>\n",
      "   __perfMark('head_tag_start');\n",
      "  </script>\n",
      "  <title>\n",
      "   Ask Reddit...\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <style>\n",
      "   /* http://meyerweb.com/eric/tools/css/reset/\n",
      "    v2.0 | 20110126\n",
      "    License: none (public domain)\n",
      "  */\n",
      "\n",
      "  html, body, div, span, applet, object, iframe,\n",
      "  h1, h2, h3, h4, h5, h6, p, blockquote, pre,\n",
      "  a, \n"
     ]
    }
   ],
   "source": [
    "# Para abrir localmente e trabalhar com o arquivo e não com vários requests:\n",
    "html = open_html('askreddit_top_day')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pode-se também abrir a página desejada direto do servidor fonte em formato html:\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "### Vale lembrar! ###\n",
    "# Esse método, pode ser danoso aos donos do site a ser requisitado, ou em alguns casos não funcionar. Um script em python, se codificado incorretamente, pode executar milhares de requests por segundo, possivelmente tirando o site do ar, e podendo causar danos financeiros aos donos do site em questão. Por conta disso, muitos sites impedem requisições consecutivas como medida de segurança."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Exemplos básicos, apenas como \"anotação\" para consulta rápida ###\n",
    "\n",
    "## Outra forma para importar uma página desejada salva num arquivo:\n",
    "\n",
    "with open('simple.html') as html_file:\n",
    "     soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "\n",
    "## Exemplos de métodos BeautifulSoup:\n",
    "\n",
    "# Mostra o primeiro título da pagina em formato de texto:\n",
    "match1 = soup.title.text\n",
    "\n",
    "# Mostra o primeiro div da pagina contendo classe, headers e paragrafos:\n",
    "match2 = soup.div\n",
    "\n",
    "# Método .find('tag') que retorna a primeira correpondência as tags podem ser 'div', 'a', 'h3', etc., e a classe pode ser 'footer', 'article', etc.:\n",
    "match3 = soup.find('div', class_='article')\n",
    "\n",
    "\n",
    "## Caso seja necessário, é possível utilizar o método try do python para testar e retornar valores, caso não se encontre um desejado resultado:\n",
    "\n",
    "try:\n",
    "    (CÓDIGO DE SCRAPPING)\n",
    "    # pass\n",
    "except Exception as e:\n",
    "    pass # Ou uma atribuição. Exemplo seria atribuir None a algum parâmetro \"raspado\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o parsing no código reddit:\n",
    "\n",
    "Primeiramente vamos analisar se o primeiro top thread é o desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## html do bloco completo contendo todos os comentários\n",
    "all_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "## html da tag e classe do número de pontos de certa thread:\n",
    "# <div class=\"_1rZYMD_4xY3gRcSS3p8ODO\" style=\"color:#1A1A1B\">22.8k</div>\n",
    "pontos_thread = all_threads.find('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "pontos_primeira_thread = pontos_thread.text\n",
    "\n",
    "## html da tag e classe do texto de uma certa thread\n",
    "# <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3>\n",
    "thread = all_threads.find('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "texto_thread = thread.text\n",
    "\n",
    "## html do link do texto\n",
    "# <a data-click-id=\"body\" class=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\" href=\"/r/AskReddit/comments/cyqtk2/teachers_of_reddit_what_was_the_most_obvious/\"><div class=\"_2SdHzo12ISmrC8H86TgSCp _3wqmjmv3tb_k-PROt7qFZe \" style=\"--posttitletextcolor:#444e59\" theme=\"[object Object]\"><h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3></div></a>\n",
    "link = all_threads.find('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "referencia = link['href']\n",
    "texto_link = f'https://www.reddit.com{referencia}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8k\n",
      "Everyone has a scar on their body from something dumb, they did as a child. What's your story?\n",
      "https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/\n"
     ]
    }
   ],
   "source": [
    "# Resultado dos pontos e texto do primeiro top thread do dia:\n",
    "print(pontos_primeira_thread)\n",
    "print(texto_thread)\n",
    "print(texto_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo certo até aqui, então vamos seguir com os próximos passos.\n",
    "\n",
    "### Realizando um loop sobre todas as top threads desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "        \n",
    "print(len(extracted_points))\n",
    "print(len(extracted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código, é possivel ver que a lista contendo as pontuações está duplicada. Provavelmente isto ocorre por haverem classes repitidas no código html. Para consertar isto, basta criarmos uma nova lista contendo os valores únicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28.8k', '28.8k', '3.3k', '3.3k', '2.7k', '2.7k', '1.2k', '1.2k', '1.5k', '1.5k', '812', '812', '936', '936', '563', '563']\n",
      "['28.8k', '3.3k', '2.7k', '1.2k', '1.5k', '812', '936', '563']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_points)\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "    \n",
    "print(unique_extracted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': \"Everyone has a scar on their body from something dumb, they did as a child. What's your story?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/'}]\n"
     ]
    }
   ],
   "source": [
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            \n",
    "print(top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opcionalmente podemos salvar o resultando em um arquivo json ou csv para uso futuro\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('data.json', 'w') as outfile:\n",
    "#     json.dump(top_threads, outfile, indent=4)\n",
    "    \n",
    "import csv\n",
    "csv_file = open('data.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['pontuacao', 'subreddit', 'thread', 'link'])\n",
    "\n",
    "# inserir csv_writer.writerow([p, s , t , l]) no loop das \"top_threads = []\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping na pagina principal\n",
    "\n",
    "Agora vamos ir para uma página mais geral do site www.reddit.com, e realizar um novo scrape (Novamente, a célula abaixo foi convertida em Raw NBConvert, para evitar rodá-la acidentalmente):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/top/'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'reddit_top_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pontuacao': '121k', 'subreddit': '/r/aww', 'titulo thread': 'Scared cat gets saved by two French guys', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyv97r/scared_cat_gets_saved_by_two_french_guys/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/memes', 'titulo thread': 'The Area 51 raid is still happening right?', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cz2i20/the_area_51_raid_is_still_happening_right/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/pics', 'titulo thread': 'In 1964, Ringo Starr snapped a photo of some high school students who skipped class to see the Beatles during their first trip to the US. The group had no idea the photo existed until Ringo published his book of photos. Nearly 50 years later, the group reunited and recreated the photo.', 'link para os comentarios': 'https://www.reddit.com/r/pics/comments/cyx1os/in_1964_ringo_starr_snapped_a_photo_of_some_high/'}\n",
      "\n",
      "{'pontuacao': '103k', 'subreddit': '/r/aww', 'titulo thread': 'Found a tiny danger noodle in need of assistance 🐍', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyx8xb/found_a_tiny_danger_noodle_in_need_of_assistance/'}\n",
      "\n",
      "{'pontuacao': '99.8k', 'subreddit': '/r/memes', 'titulo thread': 'I can feel it', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cyu0z3/i_can_feel_it/'}\n",
      "\n",
      "{'pontuacao': '84.7k', 'subreddit': '/r/MurderedByWords', 'titulo thread': \"It's never just about a cake\", 'link para os comentarios': 'https://www.reddit.com/r/MurderedByWords/comments/cyt56i/its_never_just_about_a_cake/'}\n",
      "\n",
      "{'pontuacao': '80.1k', 'subreddit': '/r/mildlyinteresting', 'titulo thread': 'This vine climbed up a chair to silence my wind chime.', 'link para os comentarios': 'https://www.reddit.com/r/mildlyinteresting/comments/cyw9xx/this_vine_climbed_up_a_chair_to_silence_my_wind/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Código restante:\n",
    "\n",
    "html = open_html('reddit_top_today')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "\n",
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            csv_writer.writerow([p, s , t , l])\n",
    "\n",
    "csv_file.close()\n",
    "for i in range(len(top_threads)):\n",
    "    print(top_threads[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping em uma lista de subreddits\n",
    "\n",
    "Agora que entendemos como realizar o scrappping em um único link, o próximo passo seria realizar o scraping a partir de uma lista de subreddits separados por ponto-e-vírgula, e.g., \"programming;dogs;brazil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "askreddit;worldnews;cats\n",
      "\n",
      " ['askreddit', 'worldnews', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# input: askreddit;worldnews;cats\n",
    "subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "subreddits_separated = subreddits.split(';')\n",
    "\n",
    "print('\\n', subreddits_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/askreddit/top/?t=day', 'https://www.reddit.com/r/worldnews/top/?t=day', 'https://www.reddit.com/r/cats/top/?t=day']\n"
     ]
    }
   ],
   "source": [
    "# A partir da lista criada anteriormente, cria-se outra lista com as urls para download do código html\n",
    "urls_subreddits = []\n",
    "for i in range(len(subreddits_separated)):\n",
    "    url_link = 'https://www.reddit.com/r/{}/top/?t=day'.format(subreddits_separated[i])\n",
    "    urls_subreddits.append(url_link)\n",
    "\n",
    "print(urls_subreddits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Aqui vamos salvar os htmls usando um for loop:\n",
    "\n",
    "# for i in range(0, len(urls_subreddits)):\n",
    "for i in range(0, 1):\n",
    "    url = urls_subreddits[i]\n",
    "    codigo_html = requests.get(url)\n",
    "    save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indo para a pasta principal, é possível ver que os arquivos foram salvos. Mas tem casos em que rodando o código acima, por conta da proteção que o site reddit possui, os htmls salvos não correspondem aos desejados. Nesses casos é necessária uma intervenção humana no looping para ser possível obter os htmls desejados, tendo que baixar um por vez.\n",
    "\n",
    "A partir da lista de htmls é possível realizar o mesmo scrapping anterior utilizando um loop, como mostrado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/askreddit:  [{'pontuacao': '33.4k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What do people THINK is a scam, but they actually just don't understand it?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0dlvo/what_do_people_think_is_a_scam_but_they_actually/'}, {'pontuacao': '30.9k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What's a negative fact about marijuana we should know?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0aoi5/whats_a_negative_fact_about_marijuana_we_should/'}, {'pontuacao': '40.2k', 'subreddit': '/r/AskReddit', 'titulo thread': \"The 2010's decade will be over in 4 months. What do you think people will remember this decade for?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0jjc2/the_2010s_decade_will_be_over_in_4_months_what_do/'}, {'pontuacao': '24.5k', 'subreddit': '/r/AskReddit', 'titulo thread': \"What's the worst case you've seen of someone trying to play the victim when it was all their own fault?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d0fwvx/whats_the_worst_case_youve_seen_of_someone_trying/'}, {'pontuacao': '9.7k', 'subreddit': '/r/AskReddit', 'titulo thread': 'What everyday thing seriously creeps you out?', 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/d08w7m/what_everyday_thing_seriously_creeps_you_out/'}]\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I’m so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    # Nesses dois casos (extracted_points e extracted_texts), é possível criarmos uma função, \n",
    "    # mas aqui como achei que o código não possui uma recorrência alta desses loops\n",
    "    # preferi deixá-los explícitos\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread) \n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "    \n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "            likes = p[0:-1]\n",
    "            likes = int(float(likes)*1000)\n",
    "            if likes >= 5000:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também modificar o limite de pontos para um mínimo de 500 pontos por exemplo. Como mostrado abaixo:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread)\n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "\n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "    \n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1: # Note que a partir daqui, o código foi modificado com o fim de obter as threads com pontuação menor que 1k\n",
    "            likes = p\n",
    "            if p[-1] == 'k':\n",
    "                likes = p[0:-1]\n",
    "                likes = int(float(likes)*1000)\n",
    "            else:\n",
    "                likes = int(likes)\n",
    "            if likes >= 500:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "    if top_threads == []:\n",
    "        top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print(unique_extracted_points)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima, juntamente com os threads, também mandei imprimir os valores dos likes para compararmos os resultados e tentar buscar por erros. Se rodarmos o código, aparentemente estará tudo certo.\n",
    "\n",
    "Nos casos em que não temos threads com o valor de pontos desejados, nosso código não retornaria resultados para esses casos, retornando ao invés disso a resposta \"Nao ha top threads com os requisitos [...]\".\n",
    "\n",
    "## Resolução \"extra\" - Função para 'reddit scrapping':\n",
    "Por fim, podemos criar uma função que recebe os subreddits desejados e retorna as top threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from save_open_html import *\n",
    "\n",
    "def top_threads(subreddits):\n",
    "#     import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "#     subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "    subreddits_separated = subreddits.split(';')\n",
    "    \n",
    "    urls_subreddits = []\n",
    "    for i in range(len(subreddits_separated)):\n",
    "        url_link = 'https://www.reddit.com/r/{}/top/?t=day'.format(subreddits_separated[i]) \n",
    "        urls_subreddits.append(url_link)\n",
    "    \n",
    "#     Aqui salvaríamos os htmls usando um for loop:\n",
    "#\n",
    "#     for i in range(0, len(urls_subreddits)):\n",
    "#         url = urls_subreddits[i]\n",
    "#         codigo_html = requests.get(url)\n",
    "#         save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "    for i in subreddits_separated:\n",
    "\n",
    "        html = open_html(f'reddit_{i}')\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "        all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "        all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "        all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "        extracted_points = []\n",
    "        for points in all_points_thread:\n",
    "            point = points.text\n",
    "            extracted_points.append(point)\n",
    "\n",
    "        extracted_texts = []\n",
    "        for threads in all_texts:\n",
    "            thread = threads.text\n",
    "            extracted_texts.append(thread)\n",
    "\n",
    "        extracted_links = []\n",
    "        for links in all_links:\n",
    "            referencia = links['href']\n",
    "            if referencia.startswith('http'):\n",
    "                extracted_links.append(referencia)\n",
    "            else:\n",
    "                texto_link = f'https://www.reddit.com{referencia}'\n",
    "                extracted_links.append(texto_link)\n",
    "\n",
    "        unique_extracted_points = []\n",
    "        for j in range(0, len(extracted_points), 2):\n",
    "            unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "        top_threads = []\n",
    "        for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "            if len(p) > 1: # Note que a partir daqui, o código foi modificado com o fim de obter as threads com pontuação menor que 1k\n",
    "                likes = p\n",
    "                if p[-1] == 'k':\n",
    "                    likes = p[0:-1]\n",
    "                    likes = int(float(likes)*1000)\n",
    "                else:\n",
    "                    likes = int(likes)\n",
    "                if likes >= 5000:\n",
    "                    subreddit = l.split('/')[4]\n",
    "                    s = f'/r/{subreddit}'\n",
    "                    record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                    top_threads.append(record)\n",
    "        if top_threads == []:\n",
    "            top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "        print()\n",
    "        print(f'Top threads de /r/{i}: ', top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top threads de /r/programming:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I’m so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n"
     ]
    }
   ],
   "source": [
    "# inputs posíveis: programming;dogs;brazil;askreddit;worldnews;cats\n",
    "top_threads('programming;cats;worldnews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolução - Parte 2: Bot para Telegram\n",
    "\n",
    "### Criando um bot\n",
    "\n",
    "A seguir, preferi colocar um código de bot (um pouco) mais completo com o intuito de reutilizar esse código futuramente (seja para revisar alguns conceitos, quanto para poder implementar os comandos após as hashtags - afinal, também me divirto trabalhando hehe)\n",
    "\n",
    "A base do código foi retirada do 'Curso de Python na prática', presente no youtube, e gravado pelo usuário 11Wills11:\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLsMpSZTgkF5C_Kkc0XBtM3OVLBsjUkjzy\n",
    "\n",
    "Esse bot é capaz de responder frases simples, tais como 'oi', 'tchau'. Também é possível ensinar novas frases utilizando o comando 'aprende'. Ele também é \"customizado\" para tratar diferentemente quem ele \"conhece\" e desconhecidos.\n",
    "\n",
    "Quem ele não conhece, ele retorna \"Muito Prazer (nome)!\", seguido de uma pergunta sobre os threads. E quem ele conhece, retorna \"oi (nome)! te adoro, sua maravilhosa!\", seguido, também, de uma pergunta sobre os threads.\n",
    "\n",
    "Após o código, existem algumas demonstrações.\n",
    "\n",
    "Segue o bot com algumas modificações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um bot básico:\n",
    "# import json\n",
    "# import sys\n",
    "# import os\n",
    "# import subprocess as sub\n",
    "from save_open_html import *\n",
    "\n",
    "class Chatbot():\n",
    "    # Na inicialização '__init__(self,nome)' desta classe, a lista de conhecidos\n",
    "    # e um histórico da conversa atual são inicializados junto do\n",
    "    # nome do Chatbot, sendo \"memorizado\" para consultas futuras.\n",
    "    def __init__(self, nome):\n",
    "#         Esses comandos após as hashtags, são comandos para implementar um aprendizado de nomes\n",
    "#\n",
    "#         try:\n",
    "#             memoria = open(nome+'.json', 'r')\n",
    "#         except FileNotFoundError:\n",
    "#             memoria = open(nome+'.json', 'w')\n",
    "#             memoria.write('[\"Criadora\", \"Lais\"], {\"oi\": \"oi! qual o seu nome?\", \"tchau\": \"tchau!\", \"nenhum\": \"ok... sinto muito :(\"}')\n",
    "#             memoria.close()\n",
    "#             memoria = open(nome+'.json', 'r')\n",
    "        self.nome = nome\n",
    "        self.conhecidos = ['Lais', 'Renata', 'Mariana']\n",
    "#         self.conhecidos, self.frases = json.load(memoria)\n",
    "#         memoria.close()\n",
    "        self.historico = [None]\n",
    "        self.frases = {'oi': 'oi! qual o seu nome?', 'tchau': 'tchau!', 'nenhum': 'ok... sinto muito :('}\n",
    "    \n",
    "    def escuta(self, frase=None):\n",
    "        if frase == None:\n",
    "            frase = input('>:')\n",
    "        frase = str(frase)\n",
    "        if 'executa ' in frase:\n",
    "            return frase\n",
    "        frase = frase.lower()\n",
    "        return frase\n",
    "    \n",
    "    def pensa(self, frase):\n",
    "        if frase in self.frases:\n",
    "            return self.frases[frase]\n",
    "        \n",
    "        if frase == 'aprende':\n",
    "#             chave = input('Digite a frase: ')\n",
    "#             resp = input('Digite a resposta: ')\n",
    "#             self.frases[chave] = resp\n",
    "#             return 'Aprendido'\n",
    "\n",
    "#       No caso do telegram, temos que substituir os comandos acima para que o bot funcione\n",
    "            return 'Digite a frase: '\n",
    "        \n",
    "#       Os comandos abaixo servem para responder frases que dependem do historico\n",
    "        ultimaFrase = self.historico[-1]\n",
    "        if ultimaFrase == 'oi! qual o seu nome?':\n",
    "            nome = self.pega_nome(frase)\n",
    "            resp = self.responde_nome(nome)\n",
    "            return resp\n",
    "        if ultimaFrase == 'Digite a frase: ':\n",
    "            self.chave = frase\n",
    "            return 'Digite a resposta: '\n",
    "        if ultimaFrase == 'Digite a resposta: ':\n",
    "            resp = frase\n",
    "            self.frases[self.chave] = resp\n",
    "#             gravaMemoria()\n",
    "            return 'Aprendido'\n",
    "\n",
    "        if '/nadaprafazer ' in frase:\n",
    "            subreddit = frase.replace('/nadaprafazer ', '')\n",
    "            try:\n",
    "                return top_threads(subreddit)\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            resp = srt(eval(frase))\n",
    "            return resp\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return 'nao te entendi o.o'\n",
    "    \n",
    "    def pega_nome(self, nome):\n",
    "        nome = nome.split(' ')\n",
    "        if len(nome) == 1:\n",
    "            nome = nome[0]\n",
    "        else:\n",
    "            nome = nome[-1]\n",
    "        nome = nome.title()\n",
    "        return nome\n",
    "    \n",
    "    def responde_nome(self, nome):\n",
    "        if nome in self.conhecidos:\n",
    "            frase = 'oi '\n",
    "#             print(frase+nome+'! te adoro, sua maravilhosa!')\n",
    "        else:\n",
    "            frase = 'Muito prazer '\n",
    "        return frase+nome+'! '\n",
    "#             Estes códigos seriam para criar um arquivo contendo todas as pessoas que o bot já conversou, para ser implementado junto dos comandos comentados acima (em __init__)\n",
    "#\n",
    "#             self.conhecidos.append(nome)\n",
    "#             memoria = open(self.nome+'.json', 'w')\n",
    "#             json.dump(self.conhecidos, memoria)\n",
    "#             memoria.close()\n",
    "#       ou\n",
    "#             self.gravaMemoria()\n",
    "            \n",
    "#         return 'O que gostaria gostaria de acompanhar hoje?' \n",
    "        \n",
    "    def fala(self, frase):\n",
    "#        Esses comandos abaixo após as hashtags seriam para implementar outra funcionalidade no bot: a de abrir documentos, programas e paginas de internet\n",
    "#        No caso do linux, podemos escrever para o bot \"executa chromium\", por exemplo, para que ele abra o navegador.\n",
    "#        Ou por exemplo \"executa https://www.google.com.br\", para abrir o site\n",
    "#\n",
    "#         if 'executa ' in frase:\n",
    "#             plataforma = sys.plataform\n",
    "#             comando = frase.replace('executa ', '')\n",
    "#             if 'win' in plataforma:\n",
    "#                 os.startfile(comando)\n",
    "#             if 'linux' in plataforma:\n",
    "#                 try:\n",
    "#                     sub.Popen(comando)\n",
    "#                 except FileNotFoundError:\n",
    "#                     sub.Popen(['xdg-open', comando])\n",
    "\n",
    "#         else:\n",
    "        print(frase)\n",
    "        self.historico.append(frase)\n",
    "        \n",
    "#         def gravaMemoria(self):\n",
    "#             memoria = open(self.nome+'.json', 'w')\n",
    "#             json.dump([self.conhecidos, self.frases], memoria)\n",
    "#             memoria.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bot funciona com a lógica: Escuta --> Pensa --> Fala.\n",
    "\n",
    "Dentro da Classe 'Chatbot', temos essas funções que o caracterizam e mais outras duas (pega_nome, responde_nome) com o intuito de otimizar a leitura do código.\n",
    "\n",
    "Para que o bot fique completo, precisamos de um loop que o mantenha \"ativo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:Lais, sua criadora\n",
      "Muito prazer Criadora!\n",
      "Quais threads gostaria de acompanhar hoje?\n",
      ">:nenhum\n",
      "ok... sinto muito :(\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "#### APENAS EXEMPLO! EVITAR RODAR A CELULA ####\n",
    "Bot = Chatbot('Red - O scrappista')\n",
    "\n",
    "while True:\n",
    "    frase = Bot.escuta()\n",
    "    resp = Bot.pensa(frase)\n",
    "    Bot.fala(resp)\n",
    "    if resp == 'tchau!':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assim, podemos criar uma função que executa o bot em questão:\n",
    "\n",
    "def executa_bot(nome_do_bot):\n",
    "    Bot = Chatbot(nome_do_bot)\n",
    "    while True:\n",
    "        frase = Bot.escuta()\n",
    "        resp = Bot.pensa(frase)\n",
    "        Bot.fala(resp)\n",
    "        if resp == 'tchau!':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:Mariana\n",
      "oi Mariana! te adoro, sua maravilhosa!\n",
      "Quais threads gostaria de acompanhar hoje?\n",
      ">:nenhum\n",
      "ok... sinto muito :(\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "### APENAS EXEMPLO! EVITAR RODAR A CELULA ###\n",
    "executa_bot('Red - O scrappista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">:oi\n",
      "oi! qual o seu nome?\n",
      ">:Laís\n",
      "Muito prazer Laís! \n",
      ">:/NadaPraFazer cats\n",
      "\n",
      "Top threads de /r/cats:  [{'pontuacao': '12.9k', 'subreddit': '/r/cats', 'titulo thread': 'Stray kitten after being de-flead, fed and bathed', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d09fx6/stray_kitten_after_being_deflead_fed_and_bathed/'}, {'pontuacao': '11.7k', 'subreddit': '/r/cats', 'titulo thread': \"Don't have many friends to share my cat with so here's how he sleeps\", 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0ewwv/im_so_lucky_to_have_her/'}, {'pontuacao': '9.1k', 'subreddit': '/r/cats', 'titulo thread': 'I’m so lucky to have her.', 'link para os comentarios': 'https://www.reddit.com/r/cats/comments/d0fovd/sweet_kitty_welcomes_returning_soldier/'}]\n",
      "None\n",
      ">:/NadaPraFazer dogs;worldnews\n",
      "\n",
      "Top threads de /r/dogs:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/worldnews:  [{'pontuacao': '33.1k', 'subreddit': '/r/worldnews', 'titulo thread': 'Robert Mugabe dies aged 95', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0c79s/robert_mugabe_dies_aged_95/'}, {'pontuacao': '30.3k', 'subreddit': '/r/worldnews', 'titulo thread': 'UK: Law to stop no-deal Brexit passed by Parliament', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0efd1/please_leave_my_town_boris_johnson_berated_by/'}, {'pontuacao': '25.5k', 'subreddit': '/r/worldnews', 'titulo thread': \"'Please leave my town': Boris Johnson berated by members of the public as his election campaign launch goes wrong\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0fkls/hm_stops_buying_leather_from_brazil_over_amazon/'}, {'pontuacao': '8.8k', 'subreddit': '/r/worldnews', 'titulo thread': 'H&M stops buying leather from Brazil over Amazon fires', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0dq0d/germanys_merkel_says_hong_kongs_rights_should_be/'}, {'pontuacao': '7.8k', 'subreddit': '/r/worldnews', 'titulo thread': \"Germany's Merkel says Hong Kong's rights should be protected\", 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0ef77/the_scientists_who_photographed_a_supermassive/'}, {'pontuacao': '5.2k', 'subreddit': '/r/worldnews', 'titulo thread': 'The scientists who photographed a supermassive black hole for the first time just won a US$3 million prize.', 'link para os comentarios': 'https://www.reddit.com/r/worldnews/comments/d0bebf/atheists_have_right_to_their_opinion_rules_indian/'}]\n",
      "None\n",
      ">:tchau\n",
      "tchau!\n"
     ]
    }
   ],
   "source": [
    "# Utilize este para \"brincar\", ou testar, o código,\n",
    "# lembrando que o bot ainda não sabe retornar as top threads :)\n",
    "\n",
    "executa_bot('Red - O scrappista')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferindo o bot para telegram\n",
    "\n",
    "Primeiramente, precisamos criar um bot no servidor do telegram. Pra isso é necessário ter uma conta e logar no telegram. Na página do telegram, na caixa de busca, escreve-se \"botfather\". Clicando no primeiro perfil, que é um bot do telegram para criar bots, vai aparecer algumas opções, dentre elas a \"/newbot\". Seguindo os passos, é possível criar o bot inicial.\n",
    "\n",
    "No meu caso, o nome escolhido foi 'RedScrappista', e o token de acesso é o:\n",
    "\n",
    "989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\n",
    "\n",
    "Vamos então importar o módulo 'telepot' e criar o bot com o token dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7ee77980169f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Para pegar as mensagens que o bot recebeu (nesse caso eu enviei um 'oi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# antes de rodar essa célula:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUpdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/__init__.py\u001b[0m in \u001b[0;36mgetUpdates\u001b[0;34m(self, offset, limit, timeout, allowed_updates)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;34m\"\"\" See: https://core.telegram.org/bots/api#getupdates \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'getUpdates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rectify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m     def setWebhook(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/__init__.py\u001b[0m in \u001b[0;36m_api_request\u001b[0;34m(self, method, params, files, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api_request_with_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/telepot/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(req, **user_kw)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0muser_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# `fn` must be thread-safe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "bot = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "\n",
    "# Para pegar as mensagens que o bot recebeu (nesse caso eu enviei um 'oi'\n",
    "# antes de rodar essa célula:\n",
    "bot.getUpdates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_id': 11,\n",
       " 'from': {'id': 989962435,\n",
       "  'is_bot': True,\n",
       "  'first_name': 'RedScrappista',\n",
       "  'username': 'RedScrappistaBot'},\n",
       " 'chat': {'id': 937485481,\n",
       "  'first_name': 'Laís',\n",
       "  'last_name': 'Alves',\n",
       "  'type': 'private'},\n",
       " 'date': 1567816816,\n",
       " 'text': 'olá lais'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para enviar mensagens, o primeiro argumento do método abaixo é o 'chat' 'id':\n",
    "bot.sendMessage(937485481, 'olá lais')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso vc tiver o telegram aberto, é possível gerar e verificar essas interações (desde que vc modifique o chat id) abrindo o bot no telegram através do link:\n",
    "\n",
    "t.me/RedScrappistaBot\n",
    "\n",
    "É possível também receber mensagens de uma forma mais fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olá, agora sao 8:05pm\n",
      "e agora se passaram 20 segundos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b04725561c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "bot = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "\n",
    "def recebendoMsg(msg):\n",
    "    print(msg['text'])\n",
    "\n",
    "bot.message_loop(recebendoMsg)\n",
    "\n",
    "while True:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabendo como se envia e se recebe mensagens através dos métodos de bot do módulo \"telepot\", vamos então integrar o código do chatbot escrito anteriormente, com a interface do telegram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e3465c8260fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telepot\n",
    "# from Chatbot import Chatbot\n",
    "telegram = telepot.Bot(\"989962435:AAHMclI0B6S7T5X88VI8ti6y4CkLN8UnJmg\")\n",
    "bot = Chatbot(\"RedScrappista\")\n",
    "\n",
    "def recebendoMsg(msg):\n",
    "    frase = bot.escuta(frase=msg['text'])\n",
    "    resp = bot.pensa(frase)\n",
    "    bot.fala(resp)\n",
    "    chatID = msg['chat']['id']\n",
    "    # Esse método abaixo é correspondente ao de cima\n",
    "    # tipoMsg, tipoChat, chatID = telepot.glance(msg) \n",
    "    telegram.sendMessage(chatID, resp)\n",
    "\n",
    "telegram.message_loop(recebendoMsg)\n",
    "\n",
    "while True:\n",
    "    pass\n",
    "# O loop acima é um loop infinito para ser executado durante a conversa sem parar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma é possível enviar e receber mensagens para o bot através do telegram.\n",
    "\n",
    "O único problema é que não consegui configurar o comando /NadaPraFazer, dentro do telegram. E quando envio o comando, aqui na minha máquina ele demora para ser processado.\n",
    "\n",
    "(Gostaria de obter ajuda dos universitários nessa rs fiquei bem curiosa como fazer ^^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
