{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafios idwall\n",
    "\n",
    "\n",
    "## Resolução da Parte 1 do Desafio 2 - Crawlers\n",
    "\n",
    "Web Scraping do Reddit, escrito em Python, utilizando as bibliotecas _request_ e _BeautifulSoup_.\n",
    "\n",
    "#### Desafio:\n",
    "Encontrar e listar as _threads_ com 5000 pontos ou mais no Reddit naquele momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução:\n",
    "\n",
    "É possível realizar o scrapping via PRAW, um wrapper para o API do Reddit, o qual permite realizar scrapings dos subreddits, criar um bot, entre outras funcionalidades.\n",
    "\n",
    "Segue um site que ensina realizar scrapping por esse método:\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "Mas aqui, com o intuito de demonstrar habilidades mais gerais, vamos realizar o scrapping utilizando os pacotes 'request' e 'BeautifulSoup'.\n",
    "\n",
    "Para uma breve introdução sobre web scraping e aplicação destes pacotes ver:\n",
    "\n",
    "https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k&t=1428s\n",
    "\n",
    "\n",
    "Dito isto, vamos começar pelo simples e buscar as _top threads_ dentro do subreddit 'r/AskReddit': https://www.reddit.com/r/AskReddit/top/?t=day\n",
    "\n",
    "\"Subreddits são como fóruns dentro do Reddit e as postagens são chamadas threads.\n",
    "\n",
    "Para quem gosta de gatos, há o subreddit '/r/cats' com threads contendo fotos de gatos fofinhos. Para threads sobre o Brasil, vale a pena visitar '/r/brazil' ou ainda '/r/worldnews'. Um dos maiores subreddits é o '/r/AskReddit'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a url e salvando o arquivo em html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importam-se as bibliotecas necessárias para o scrapping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# E cria-se uma função para salvar e outra para\n",
    "# abrir a página html, a fim de minimizar danos ao servidor:\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATENÇÃO!!!\n",
    "\n",
    "Recomenda-se não rodar o código da célula abaixo, sendo utilizado assim apenas para simples conferência.\n",
    "\n",
    "Como o reddit possui um sistema automatizado que impede mais que um request a cada dois segundos é possível que o código abaixo gere um \"erro\", de forma a não ser capaz de obter o código html do site. Eu tentei procurar entender o porquê, mas não obtive uma resposta.\n",
    "\n",
    "Mas caso queira rodar o código, é necessário tentar algumas vezes, caso não consiga de primeira, até conseguir obter o html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Por segurança, célula com formatação Raw NBConvert ###\n",
    "\n",
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/r/AskReddit/top/?t=day'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'askreddit_top_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   var __SUPPORTS_TIMING_API = typeof performance === 'object' && !!performance.mark && !! performance.measure && !!performance.getEntriesByType;\n",
      "          function __perfMark(name) { __SUPPORTS_TIMING_API && performance.mark(name); };\n",
      "          var __firstLoaded = false;\n",
      "          function __markFirstPostVisible() {\n",
      "            if (__firstLoaded) { return; }\n",
      "            __firstLoaded = true;\n",
      "            __perfMark(\"first_post_title_image_loaded\");\n",
      "          }\n",
      "  </script>\n",
      "  <script>\n",
      "   __perfMark('head_tag_start');\n",
      "  </script>\n",
      "  <title>\n",
      "   Ask Reddit...\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <style>\n",
      "   /* http://meyerweb.com/eric/tools/css/reset/\n",
      "    v2.0 | 20110126\n",
      "    License: none (public domain)\n",
      "  */\n",
      "\n",
      "  html, body, div, span, applet, object, iframe,\n",
      "  h1, h2, h3, h4, h5, h6, p, blockquote, pre,\n",
      "  a, \n"
     ]
    }
   ],
   "source": [
    "# Para abrir localmente e trabalhar com o arquivo e não com vários requests:\n",
    "html = open_html('askreddit_top_day')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pode-se também abrir a página desejada direto do servidor fonte em formato html:\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "### Vale lembrar! ###\n",
    "# Esse método, pode ser danoso aos donos do site a ser requisitado, ou em alguns casos não funcionar. Um script em python, se codificado incorretamente, pode executar milhares de requests por segundo, possivelmente tirando o site do ar, e podendo causar danos financeiros aos donos do site em questão. Por conta disso, muitos sites impedem requisições consecutivas como medida de segurança."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Exemplos básicos, apenas como \"anotação\" para consulta rápida ###\n",
    "\n",
    "# Outra forma para importar uma página desejada salva num arquivo:\n",
    "\n",
    "with open('simple.html') as html_file:\n",
    "     soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "\n",
    "# Exemplos de métodos BeautifulSoup:\n",
    "\n",
    "# Mostra o primeiro título da pagina em formato de texto:\n",
    "match1 = soup.title.text\n",
    "\n",
    "# Mostra o primeiro div da pagina contendo classe, headers e paragrafos:\n",
    "match2 = soup.div\n",
    "\n",
    "# Método .find('tag') que retorna a primeira correpondência as tags podem ser 'div', 'a', 'h3', etc., e a classe pode ser 'footer', 'article', etc.:\n",
    "match3 = soup.find('div', class_='article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o parsing no código reddit:\n",
    "\n",
    "Primeiramente vamos analisar se o primeiro top thread é o desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## html do bloco completo contendo todos os comentários\n",
    "all_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "## html da tag e classe do número de pontos de certa thread:\n",
    "# <div class=\"_1rZYMD_4xY3gRcSS3p8ODO\" style=\"color:#1A1A1B\">22.8k</div>\n",
    "pontos_thread = all_threads.find('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "pontos_primeira_thread = pontos_thread.text\n",
    "\n",
    "## html da tag e classe do texto de uma certa thread\n",
    "# <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3>\n",
    "thread = all_threads.find('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "texto_thread = thread.text\n",
    "\n",
    "## html do link do texto\n",
    "# <a data-click-id=\"body\" class=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\" href=\"/r/AskReddit/comments/cyqtk2/teachers_of_reddit_what_was_the_most_obvious/\"><div class=\"_2SdHzo12ISmrC8H86TgSCp _3wqmjmv3tb_k-PROt7qFZe \" style=\"--posttitletextcolor:#444e59\" theme=\"[object Object]\"><h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3></div></a>\n",
    "link = all_threads.find('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "referencia = link['href']\n",
    "texto_link = f'https://www.reddit.com{referencia}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8k\n",
      "Everyone has a scar on their body from something dumb, they did as a child. What's your story?\n",
      "https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/\n"
     ]
    }
   ],
   "source": [
    "# Resultado dos pontos e texto do primeiro top thread do dia:\n",
    "print(pontos_primeira_thread)\n",
    "print(texto_thread)\n",
    "print(texto_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo certo até aqui, então vamos seguir com os próximos passos.\n",
    "\n",
    "### Realizando um loop sobre todas as top threads desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': \"Everyone has a scar on their body from something dumb, they did as a child. What's your story?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/'}, {'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': 'What is the scariest/creepiest/most disturbing thing you have ever encountered? [Serious]', 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cyv6za/what_is_the_scariestcreepiestmost_disturbing/'}]\n"
     ]
    }
   ],
   "source": [
    "top_threads = []\n",
    "for p, t, l in zip(extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "    \n",
    "print(top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcionalmente podemos salvar o resultando em um arquivo json para uso futuro\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(top_threads, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping na pagina principal\n",
    "\n",
    "Agora vamos ir para uma página mais geral do site www.reddit.com, e realizar um novo scrape (Novamente, a célula abaixo foi convertida em Raw NBConvert, para evitar rodá-la acidentalmente):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/top/'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'reddit_top_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pontuacao': '121k', 'subreddit': '/r/aww', 'titulo thread': 'Scared cat gets saved by two French guys', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyv97r/scared_cat_gets_saved_by_two_french_guys/'}\n",
      "\n",
      "{'pontuacao': '121k', 'subreddit': '/r/memes', 'titulo thread': 'The Area 51 raid is still happening right?', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cz2i20/the_area_51_raid_is_still_happening_right/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/pics', 'titulo thread': 'In 1964, Ringo Starr snapped a photo of some high school students who skipped class to see the Beatles during their first trip to the US. The group had no idea the photo existed until Ringo published his book of photos. Nearly 50 years later, the group reunited and recreated the photo.', 'link para os comentarios': 'https://www.reddit.com/r/pics/comments/cyx1os/in_1964_ringo_starr_snapped_a_photo_of_some_high/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/aww', 'titulo thread': 'Found a tiny danger noodle in need of assistance 🐍', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyx8xb/found_a_tiny_danger_noodle_in_need_of_assistance/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/memes', 'titulo thread': 'I can feel it', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cyu0z3/i_can_feel_it/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/MurderedByWords', 'titulo thread': \"It's never just about a cake\", 'link para os comentarios': 'https://www.reddit.com/r/MurderedByWords/comments/cyt56i/its_never_just_about_a_cake/'}\n",
      "\n",
      "{'pontuacao': '103k', 'subreddit': '/r/mildlyinteresting', 'titulo thread': 'This vine climbed up a chair to silence my wind chime.', 'link para os comentarios': 'https://www.reddit.com/r/mildlyinteresting/comments/cyw9xx/this_vine_climbed_up_a_chair_to_silence_my_wind/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Código restante:\n",
    "\n",
    "html = open_html('reddit_top_today')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "\n",
    "top_threads = []\n",
    "for p, t, l in zip(extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "\n",
    "for i in range(len(top_threads)):\n",
    "    print(top_threads[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping em uma lista de subreddits\n",
    "\n",
    "Agora que entendemos como realizar o scrappping em um único link, o próximo passo seria realizar o scraping a partir de uma lista de subreddits separados por ponto-e-vírgula, e.g., \"programming;dogs;brazil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "programming;dogs;brazil\n",
      "\n",
      " ['programming', 'dogs', 'brazil']\n"
     ]
    }
   ],
   "source": [
    "# input: programming;dogs;brazil\n",
    "subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "subreddits_separated = subreddits.split(';')\n",
    "\n",
    "print('\\n', subreddits_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/programming', 'https://www.reddit.com/r/dogs', 'https://www.reddit.com/r/brazil']\n"
     ]
    }
   ],
   "source": [
    "urls_subreddits = []\n",
    "for i in range(len(subreddits_separated)):\n",
    "    url_link = 'https://www.reddit.com/r/'+subreddits_separated[i]\n",
    "    urls_subreddits.append(url_link)\n",
    "\n",
    "print(urls_subreddits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(2, len(urls_subreddits)):\n",
    "    url = urls_subreddits[i]\n",
    "    codigo_html = requests.get(url)\n",
    "    save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "html = open_html('reddit_brazil')\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indo para a pasta principal, é possível ver que os arquivos foram salvos, mas por conta da proteção que o site reddit possui, os htmls salvos não correspondem aos desejados.\n",
    "\n",
    "Por conta disto, foi necessário uma intervenção humana no looping para que fosse possível obter os htmls desejados.\n",
    "\n",
    "Caso a célula acima funcionasse sem problemas, seria possível realizar o mesmo scrapping anterior utilizando um loop do código anterior, como mostrado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/programming:  []\n",
      "\n",
      "Top threads de /r/dogs:  []\n",
      "\n",
      "Top threads de /r/brazil:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread)\n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "\n",
    "    top_threads = []\n",
    "    for p, t, l in zip(extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "            likes = p[0:-1]\n",
    "            likes = int(float(likes)*1000)\n",
    "            if likes >= 5000:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos casos acima, não houveram threads com mais de 5000 pontos. Além disto, foi possível rodar o código por conta da \"intervenção\" comentada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
