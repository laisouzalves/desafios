{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafios idwall\n",
    "\n",
    "\n",
    "## Resolu√ß√£o da Parte 1 do Desafio 2 - Crawlers\n",
    "\n",
    "Web Scraping do Reddit, escrito em Python, utilizando as bibliotecas _request_ e _BeautifulSoup_.\n",
    "\n",
    "#### Desafio:\n",
    "Encontrar e listar as _threads_ com 5000 pontos ou mais no Reddit naquele momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolu√ß√£o:\n",
    "\n",
    "√â poss√≠vel realizar o scrapping via PRAW, um wrapper para o API do Reddit, o qual permite realizar scrapings dos subreddits, criar um bot, entre outras funcionalidades.\n",
    "\n",
    "Seguem dois sites que ensinam como realizar scrapping por esse m√©todo:\n",
    "\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "\n",
    "Mas aqui, com o intuito de demonstrar habilidades mais gerais, vamos realizar o scrapping utilizando os pacotes 'request' e 'BeautifulSoup'.\n",
    "\n",
    "Para uma breve introdu√ß√£o sobre web scraping e aplica√ß√£o destes pacotes ver:\n",
    "\n",
    "https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k&t=1428s\n",
    "\n",
    "\n",
    "Dito isto, vamos come√ßar pelo simples e buscar as _top threads_ dentro do subreddit 'r/AskReddit': https://www.reddit.com/r/AskReddit/top/?t=day\n",
    "\n",
    "\"Subreddits s√£o como f√≥runs dentro do Reddit e as postagens s√£o chamadas threads.\n",
    "\n",
    "Para quem gosta de gatos, h√° o subreddit '/r/cats' com threads contendo fotos de gatos fofinhos. Para threads sobre o Brasil, vale a pena visitar '/r/brazil' ou ainda '/r/worldnews'. Um dos maiores subreddits √© o '/r/AskReddit'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a url e salvando o arquivo em html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importam-se as bibliotecas necess√°rias para o scrapping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# E cria-se uma fun√ß√£o para salvar e outra para\n",
    "# abrir a p√°gina html, a fim de minimizar danos ao servidor:\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATEN√á√ÉO!!!\n",
    "\n",
    "Recomenda-se n√£o rodar o c√≥digo da c√©lula abaixo, sendo utilizado assim apenas para simples confer√™ncia.\n",
    "\n",
    "Como o reddit possui um sistema automatizado que impede mais que um request a cada dois segundos √© poss√≠vel que o c√≥digo abaixo gere um \"erro\", de forma a n√£o ser capaz de obter o c√≥digo html do site. Eu tentei procurar entender o porqu√™, mas n√£o obtive uma resposta.\n",
    "\n",
    "Mas caso queira rodar o c√≥digo, √© necess√°rio tentar algumas vezes, caso n√£o consiga de primeira, at√© conseguir obter o html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Por seguran√ßa, c√©lula com formata√ß√£o Raw NBConvert ###\n",
    "\n",
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/r/AskReddit/top/?t=day'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'askreddit_top_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   var __SUPPORTS_TIMING_API = typeof performance === 'object' && !!performance.mark && !! performance.measure && !!performance.getEntriesByType;\n",
      "          function __perfMark(name) { __SUPPORTS_TIMING_API && performance.mark(name); };\n",
      "          var __firstLoaded = false;\n",
      "          function __markFirstPostVisible() {\n",
      "            if (__firstLoaded) { return; }\n",
      "            __firstLoaded = true;\n",
      "            __perfMark(\"first_post_title_image_loaded\");\n",
      "          }\n",
      "  </script>\n",
      "  <script>\n",
      "   __perfMark('head_tag_start');\n",
      "  </script>\n",
      "  <title>\n",
      "   Ask Reddit...\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <style>\n",
      "   /* http://meyerweb.com/eric/tools/css/reset/\n",
      "    v2.0 | 20110126\n",
      "    License: none (public domain)\n",
      "  */\n",
      "\n",
      "  html, body, div, span, applet, object, iframe,\n",
      "  h1, h2, h3, h4, h5, h6, p, blockquote, pre,\n",
      "  a, \n"
     ]
    }
   ],
   "source": [
    "# Para abrir localmente e trabalhar com o arquivo e n√£o com v√°rios requests:\n",
    "html = open_html('askreddit_top_day')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pode-se tamb√©m abrir a p√°gina desejada direto do servidor fonte em formato html:\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "### Vale lembrar! ###\n",
    "# Esse m√©todo, pode ser danoso aos donos do site a ser requisitado, ou em alguns casos n√£o funcionar. Um script em python, se codificado incorretamente, pode executar milhares de requests por segundo, possivelmente tirando o site do ar, e podendo causar danos financeiros aos donos do site em quest√£o. Por conta disso, muitos sites impedem requisi√ß√µes consecutivas como medida de seguran√ßa."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Exemplos b√°sicos, apenas como \"anota√ß√£o\" para consulta r√°pida ###\n",
    "\n",
    "## Outra forma para importar uma p√°gina desejada salva num arquivo:\n",
    "\n",
    "with open('simple.html') as html_file:\n",
    "     soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "\n",
    "## Exemplos de m√©todos BeautifulSoup:\n",
    "\n",
    "# Mostra o primeiro t√≠tulo da pagina em formato de texto:\n",
    "match1 = soup.title.text\n",
    "\n",
    "# Mostra o primeiro div da pagina contendo classe, headers e paragrafos:\n",
    "match2 = soup.div\n",
    "\n",
    "# M√©todo .find('tag') que retorna a primeira correpond√™ncia as tags podem ser 'div', 'a', 'h3', etc., e a classe pode ser 'footer', 'article', etc.:\n",
    "match3 = soup.find('div', class_='article')\n",
    "\n",
    "\n",
    "## Caso seja necess√°rio, √© poss√≠vel utilizar o m√©todo try do python para testar e retornar valores, caso n√£o se encontre um desejado resultado:\n",
    "\n",
    "try:\n",
    "    (C√ìDIGO DE SCRAPPING)\n",
    "    # pass\n",
    "except Exception as e:\n",
    "    pass # Ou uma atribui√ß√£o. Exemplo seria atribuir None a algum par√¢metro \"raspado\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o parsing no c√≥digo reddit:\n",
    "\n",
    "Primeiramente vamos analisar se o primeiro top thread √© o desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## html do bloco completo contendo todos os coment√°rios\n",
    "all_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "## html da tag e classe do n√∫mero de pontos de certa thread:\n",
    "# <div class=\"_1rZYMD_4xY3gRcSS3p8ODO\" style=\"color:#1A1A1B\">22.8k</div>\n",
    "pontos_thread = all_threads.find('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "pontos_primeira_thread = pontos_thread.text\n",
    "\n",
    "## html da tag e classe do texto de uma certa thread\n",
    "# <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3>\n",
    "thread = all_threads.find('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "texto_thread = thread.text\n",
    "\n",
    "## html do link do texto\n",
    "# <a data-click-id=\"body\" class=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\" href=\"/r/AskReddit/comments/cyqtk2/teachers_of_reddit_what_was_the_most_obvious/\"><div class=\"_2SdHzo12ISmrC8H86TgSCp _3wqmjmv3tb_k-PROt7qFZe \" style=\"--posttitletextcolor:#444e59\" theme=\"[object Object]\"><h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3></div></a>\n",
    "link = all_threads.find('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "referencia = link['href']\n",
    "texto_link = f'https://www.reddit.com{referencia}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8k\n",
      "Everyone has a scar on their body from something dumb, they did as a child. What's your story?\n",
      "https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/\n"
     ]
    }
   ],
   "source": [
    "# Resultado dos pontos e texto do primeiro top thread do dia:\n",
    "print(pontos_primeira_thread)\n",
    "print(texto_thread)\n",
    "print(texto_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo certo at√© aqui, ent√£o vamos seguir com os pr√≥ximos passos.\n",
    "\n",
    "### Realizando um loop sobre todas as top threads desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "        \n",
    "print(len(extracted_points))\n",
    "print(len(extracted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse c√≥digo, √© possivel ver que a lista contendo as pontua√ß√µes est√° duplicada. Provavelmente isto ocorre por haverem classes repitidas no c√≥digo html. Para consertar isto, basta criarmos uma nova lista contendo os valores √∫nicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28.8k', '28.8k', '3.3k', '3.3k', '2.7k', '2.7k', '1.2k', '1.2k', '1.5k', '1.5k', '812', '812', '936', '936', '563', '563']\n",
      "['28.8k', '3.3k', '2.7k', '1.2k', '1.5k', '812', '936', '563']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_points)\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "    \n",
    "print(unique_extracted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': \"Everyone has a scar on their body from something dumb, they did as a child. What's your story?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/'}]\n"
     ]
    }
   ],
   "source": [
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            \n",
    "print(top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opcionalmente podemos salvar o resultando em um arquivo json ou csv para uso futuro\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(top_threads, outfile, indent=4)\n",
    "    \n",
    "import csv\n",
    "csv_file = open('data.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['pontuacao', 'subreddit', 'thread', 'link'])\n",
    "\n",
    "# inserir csv_writer.writerow([p, s , t , l]) no loop das \"top_threads = []\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping na pagina principal\n",
    "\n",
    "Agora vamos ir para uma p√°gina mais geral do site www.reddit.com, e realizar um novo scrape (Novamente, a c√©lula abaixo foi convertida em Raw NBConvert, para evitar rod√°-la acidentalmente):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/top/'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'reddit_top_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pontuacao': '121k', 'subreddit': '/r/aww', 'titulo thread': 'Scared cat gets saved by two French guys', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyv97r/scared_cat_gets_saved_by_two_french_guys/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/memes', 'titulo thread': 'The Area 51 raid is still happening right?', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cz2i20/the_area_51_raid_is_still_happening_right/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/pics', 'titulo thread': 'In 1964, Ringo Starr snapped a photo of some high school students who skipped class to see the Beatles during their first trip to the US. The group had no idea the photo existed until Ringo published his book of photos. Nearly 50 years later, the group reunited and recreated the photo.', 'link para os comentarios': 'https://www.reddit.com/r/pics/comments/cyx1os/in_1964_ringo_starr_snapped_a_photo_of_some_high/'}\n",
      "\n",
      "{'pontuacao': '103k', 'subreddit': '/r/aww', 'titulo thread': 'Found a tiny danger noodle in need of assistance üêç', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyx8xb/found_a_tiny_danger_noodle_in_need_of_assistance/'}\n",
      "\n",
      "{'pontuacao': '99.8k', 'subreddit': '/r/memes', 'titulo thread': 'I can feel it', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cyu0z3/i_can_feel_it/'}\n",
      "\n",
      "{'pontuacao': '84.7k', 'subreddit': '/r/MurderedByWords', 'titulo thread': \"It's never just about a cake\", 'link para os comentarios': 'https://www.reddit.com/r/MurderedByWords/comments/cyt56i/its_never_just_about_a_cake/'}\n",
      "\n",
      "{'pontuacao': '80.1k', 'subreddit': '/r/mildlyinteresting', 'titulo thread': 'This vine climbed up a chair to silence my wind chime.', 'link para os comentarios': 'https://www.reddit.com/r/mildlyinteresting/comments/cyw9xx/this_vine_climbed_up_a_chair_to_silence_my_wind/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C√≥digo restante:\n",
    "\n",
    "html = open_html('reddit_top_today')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "\n",
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            csv_writer.writerow([p, s , t , l])\n",
    "\n",
    "csv_file.close()\n",
    "for i in range(len(top_threads)):\n",
    "    print(top_threads[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping em uma lista de subreddits\n",
    "\n",
    "Agora que entendemos como realizar o scrappping em um √∫nico link, o pr√≥ximo passo seria realizar o scraping a partir de uma lista de subreddits separados por ponto-e-v√≠rgula, e.g., \"programming;dogs;brazil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "programming;dogs;brazil\n",
      "\n",
      " ['programming', 'dogs', 'brazil']\n"
     ]
    }
   ],
   "source": [
    "# input: programming;dogs;brazil\n",
    "subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "subreddits_separated = subreddits.split(';')\n",
    "\n",
    "print('\\n', subreddits_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/programming', 'https://www.reddit.com/r/dogs', 'https://www.reddit.com/r/brazil']\n"
     ]
    }
   ],
   "source": [
    "# A partir da lista criada anteriormente, cria-se outra lista com as urls para download do c√≥digo html\n",
    "urls_subreddits = []\n",
    "for i in range(len(subreddits_separated)):\n",
    "    url_link = 'https://www.reddit.com/r/'+subreddits_separated[i]\n",
    "    urls_subreddits.append(url_link)\n",
    "\n",
    "print(urls_subreddits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Aqui vamos salvar os htmls usando um for loop:\n",
    "\n",
    "for i in range(0, len(urls_subreddits)):\n",
    "    url = urls_subreddits[i]\n",
    "    codigo_html = requests.get(url)\n",
    "    save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "\n",
    "# html = open_html('reddit_brazil')\n",
    "# print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indo para a pasta principal, √© poss√≠vel ver que os arquivos foram salvos. Mas caso rod√°ssemos o c√≥digo acima, por conta da prote√ß√£o que o site reddit possui, os htmls salvos n√£o corresponderiam aos desejados. Por conta disto, foi necess√°rio uma interven√ß√£o humana no looping para que fosse poss√≠vel obter os htmls desejados, tendo que baixar um por vez.\n",
    "\n",
    "A partir da lista de htmls √© poss√≠vel realizar o mesmo scrapping anterior utilizando um loop do c√≥digo anterior, como mostrado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/programming:  []\n",
      "\n",
      "Top threads de /r/dogs:  []\n",
      "\n",
      "Top threads de /r/brazil:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    # Nesses dois casos (extracted_points e extracted_texts), √© poss√≠vel criarmos uma fun√ß√£o, \n",
    "    # mas aqui como achei que o c√≥digo n√£o possui uma recorr√™ncia alta desses loops\n",
    "    # preferi deix√°-los expl√≠citos\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread) \n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "    \n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1 and p[-1] == 'k': # condi√ß√£o para evitar promoted threads e threads com menos de 1000 pontos\n",
    "            likes = p[0:-1]\n",
    "            likes = int(float(likes)*1000)\n",
    "            if likes >= 5000:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos casos acima, n√£o houveram threads com mais de 5000 pontos. Al√©m disto, foi poss√≠vel rodar o c√≥digo por conta da \"interven√ß√£o\" comentada anteriormente.\n",
    "\n",
    "Caso modific√°ssemos o limite de 5000 pontos, para 500 pontos por exemplo, podemos obter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/programming:  [{'pontuacao': '2.1k', 'subreddit': '/r/programming', 'titulo thread': 'Former Google engineer breaks down interview problems he uses to screen candidates. Lots of good coding, algorithms, and interview tips.', 'link para os comentarios': 'https://www.reddit.com/r/programming/comments/cz6f5r/former_google_engineer_breaks_down_interview/'}]\n",
      "['2.1k', '438', '62', '77', '14', '12', '10', '16']\n",
      "\n",
      "Top threads de /r/dogs:  [{'pontuacao': '558', 'subreddit': '/r/dogs', 'titulo thread': '[RIP] Joey, beagle, 14 y.o.', 'link para os comentarios': 'https://www.reddit.com/r/dogs/comments/cz54pu/rip_joey_beagle_14_yo/'}, {'pontuacao': '999', 'subreddit': '/r/dogs', 'titulo thread': '[RIP] Tutya, pug, 4 y.o.', 'link para os comentarios': 'https://www.reddit.com/r/dogs/comments/cyst1s/rip_tutya_pug_4_yo/'}]\n",
      "['4', '9', '558', '85', '42', '16', '999', '24']\n",
      "\n",
      "Top threads de /r/brazil:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "['20', '24', '11', '4', '25', '65', '8', '44']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread)\n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "\n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "    \n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1: # Note que a partir daqui, o c√≥digo foi modificado com o fim de obter as threads com pontua√ß√£o menor que 1k\n",
    "            likes = p\n",
    "            if p[-1] == 'k':\n",
    "                likes = p[0:-1]\n",
    "                likes = int(float(likes)*1000)\n",
    "            else:\n",
    "                likes = int(likes)\n",
    "            if likes >= 500:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "    if top_threads == []:\n",
    "        top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print(unique_extracted_points)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima, juntamente com os threads, tamb√©m mandei imprimir os valores dos likes. Ao visitarmos as duas √∫ltimas  p√°ginas ('r/dogs', 'r/brazil'), vemos que realmente n√£o existem muitos pontos nos top di√°rios. Assim, no caso em que n√£o temos threads com o valor de pontos desejados, nosso c√≥digo acaba n√£o retornando valores/resultados para esses casos.\n",
    "\n",
    "## Resolu√ß√£o extra - Fun√ß√£o para 'reddit scrapping':\n",
    "Por fim, podemos criar uma fun√ß√£o que recebe os subreddits desejados e retorna as top threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_threads():\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "    subreddits_separated = subreddits.split(';')\n",
    "    \n",
    "    urls_subreddits = []\n",
    "    for i in range(len(subreddits_separated)):\n",
    "        url_link = 'https://www.reddit.com/r/' + subreddits_separated[i]\n",
    "        urls_subreddits.append(url_link)\n",
    "    \n",
    "#     Aqui salvar√≠amos os htmls usando um for loop:\n",
    "#     for i in range(0, len(urls_subreddits)):\n",
    "#         url = urls_subreddits[i]\n",
    "#         codigo_html = requests.get(url)\n",
    "#         save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "    for i in subreddits_separated:\n",
    "\n",
    "        html = open_html(f'reddit_{i}')\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "        all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "        all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "        all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "        extracted_points = []\n",
    "        for points in all_points_thread:\n",
    "            point = points.text\n",
    "            extracted_points.append(point)\n",
    "\n",
    "        extracted_texts = []\n",
    "        for threads in all_texts:\n",
    "            thread = threads.text\n",
    "            extracted_texts.append(thread)\n",
    "\n",
    "        extracted_links = []\n",
    "        for links in all_links:\n",
    "            referencia = links['href']\n",
    "            if referencia.startswith('http'):\n",
    "                extracted_links.append(referencia)\n",
    "            else:\n",
    "                texto_link = f'https://www.reddit.com{referencia}'\n",
    "                extracted_links.append(texto_link)\n",
    "\n",
    "        unique_extracted_points = []\n",
    "        for j in range(0, len(extracted_points), 2):\n",
    "            unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "        top_threads = []\n",
    "        for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "            if len(p) > 1: # Note que a partir daqui, o c√≥digo foi modificado com o fim de obter as threads com pontua√ß√£o menor que 1k\n",
    "                likes = p\n",
    "                if p[-1] == 'k':\n",
    "                    likes = p[0:-1]\n",
    "                    likes = int(float(likes)*1000)\n",
    "                else:\n",
    "                    likes = int(likes)\n",
    "                if likes >= 5000:\n",
    "                    subreddit = l.split('/')[4]\n",
    "                    s = f'/r/{subreddit}'\n",
    "                    record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                    top_threads.append(record)\n",
    "        if top_threads == []:\n",
    "            top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "        print()\n",
    "        print(f'Top threads de /r/{i}: ', top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "programming;dogs;brazil\n",
      "\n",
      "Top threads de /r/programming:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/dogs:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/brazil:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n"
     ]
    }
   ],
   "source": [
    "# input: programming;dogs;brazil\n",
    "top_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
