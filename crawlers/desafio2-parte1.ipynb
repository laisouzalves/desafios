{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafios idwall\n",
    "\n",
    "\n",
    "## Resolução da Parte 1 do Desafio 2 - Crawlers\n",
    "\n",
    "Web Scraping do Reddit, escrito em Python, utilizando as bibliotecas _request_ e _BeautifulSoup_.\n",
    "\n",
    "#### Desafio:\n",
    "Encontrar e listar as _threads_ com 5000 pontos ou mais no Reddit naquele momento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolução:\n",
    "\n",
    "É possível realizar o scrapping via PRAW, um wrapper para o API do Reddit, o qual permite realizar scrapings dos subreddits, criar um bot, entre outras funcionalidades.\n",
    "\n",
    "Seguem dois sites que ensinam como realizar scrapping por esse método:\n",
    "\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "\n",
    "Mas aqui, com o intuito de demonstrar habilidades mais gerais, vamos realizar o scrapping utilizando os pacotes 'request' e 'BeautifulSoup'.\n",
    "\n",
    "Para uma breve introdução sobre web scraping e aplicação destes pacotes ver:\n",
    "\n",
    "https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k&t=1428s\n",
    "\n",
    "\n",
    "Dito isto, vamos começar pelo simples e buscar as _top threads_ dentro do subreddit 'r/AskReddit': https://www.reddit.com/r/AskReddit/top/?t=day\n",
    "\n",
    "\"Subreddits são como fóruns dentro do Reddit e as postagens são chamadas threads.\n",
    "\n",
    "Para quem gosta de gatos, há o subreddit '/r/cats' com threads contendo fotos de gatos fofinhos. Para threads sobre o Brasil, vale a pena visitar '/r/brazil' ou ainda '/r/worldnews'. Um dos maiores subreddits é o '/r/AskReddit'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo a url e salvando o arquivo em html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importam-se as bibliotecas necessárias para o scrapping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# E cria-se uma função para salvar e outra para\n",
    "# abrir a página html, a fim de minimizar danos ao servidor:\n",
    "def save_html(html, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(html)\n",
    "\n",
    "def open_html(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATENÇÃO!!!\n",
    "\n",
    "Recomenda-se não rodar o código da célula abaixo, sendo utilizado assim apenas para simples conferência.\n",
    "\n",
    "Como o reddit possui um sistema automatizado que impede mais que um request a cada dois segundos é possível que o código abaixo gere um \"erro\", de forma a não ser capaz de obter o código html do site. Eu tentei procurar entender o porquê, mas não obtive uma resposta.\n",
    "\n",
    "Mas caso queira rodar o código, é necessário tentar algumas vezes, caso não consiga de primeira, até conseguir obter o html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Por segurança, célula com formatação Raw NBConvert ###\n",
    "\n",
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/r/AskReddit/top/?t=day'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'askreddit_top_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   var __SUPPORTS_TIMING_API = typeof performance === 'object' && !!performance.mark && !! performance.measure && !!performance.getEntriesByType;\n",
      "          function __perfMark(name) { __SUPPORTS_TIMING_API && performance.mark(name); };\n",
      "          var __firstLoaded = false;\n",
      "          function __markFirstPostVisible() {\n",
      "            if (__firstLoaded) { return; }\n",
      "            __firstLoaded = true;\n",
      "            __perfMark(\"first_post_title_image_loaded\");\n",
      "          }\n",
      "  </script>\n",
      "  <script>\n",
      "   __perfMark('head_tag_start');\n",
      "  </script>\n",
      "  <title>\n",
      "   Ask Reddit...\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n",
      "  <style>\n",
      "   /* http://meyerweb.com/eric/tools/css/reset/\n",
      "    v2.0 | 20110126\n",
      "    License: none (public domain)\n",
      "  */\n",
      "\n",
      "  html, body, div, span, applet, object, iframe,\n",
      "  h1, h2, h3, h4, h5, h6, p, blockquote, pre,\n",
      "  a, \n"
     ]
    }
   ],
   "source": [
    "# Para abrir localmente e trabalhar com o arquivo e não com vários requests:\n",
    "html = open_html('askreddit_top_day')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pode-se também abrir a página desejada direto do servidor fonte em formato html:\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "print(soup.prettify()[:100])\n",
    "\n",
    "### Vale lembrar! ###\n",
    "# Esse método, pode ser danoso aos donos do site a ser requisitado, ou em alguns casos não funcionar. Um script em python, se codificado incorretamente, pode executar milhares de requests por segundo, possivelmente tirando o site do ar, e podendo causar danos financeiros aos donos do site em questão. Por conta disso, muitos sites impedem requisições consecutivas como medida de segurança."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Exemplos básicos, apenas como \"anotação\" para consulta rápida ###\n",
    "\n",
    "## Outra forma para importar uma página desejada salva num arquivo:\n",
    "\n",
    "with open('simple.html') as html_file:\n",
    "     soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "\n",
    "## Exemplos de métodos BeautifulSoup:\n",
    "\n",
    "# Mostra o primeiro título da pagina em formato de texto:\n",
    "match1 = soup.title.text\n",
    "\n",
    "# Mostra o primeiro div da pagina contendo classe, headers e paragrafos:\n",
    "match2 = soup.div\n",
    "\n",
    "# Método .find('tag') que retorna a primeira correpondência as tags podem ser 'div', 'a', 'h3', etc., e a classe pode ser 'footer', 'article', etc.:\n",
    "match3 = soup.find('div', class_='article')\n",
    "\n",
    "\n",
    "## Caso seja necessário, é possível utilizar o método try do python para testar e retornar valores, caso não se encontre um desejado resultado:\n",
    "\n",
    "try:\n",
    "    (CÓDIGO DE SCRAPPING)\n",
    "    # pass\n",
    "except Exception as e:\n",
    "    pass # Ou uma atribuição. Exemplo seria atribuir None a algum parâmetro \"raspado\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o parsing no código reddit:\n",
    "\n",
    "Primeiramente vamos analisar se o primeiro top thread é o desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## html do bloco completo contendo todos os comentários\n",
    "all_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "## html da tag e classe do número de pontos de certa thread:\n",
    "# <div class=\"_1rZYMD_4xY3gRcSS3p8ODO\" style=\"color:#1A1A1B\">22.8k</div>\n",
    "pontos_thread = all_threads.find('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "pontos_primeira_thread = pontos_thread.text\n",
    "\n",
    "## html da tag e classe do texto de uma certa thread\n",
    "# <h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3>\n",
    "thread = all_threads.find('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "texto_thread = thread.text\n",
    "\n",
    "## html do link do texto\n",
    "# <a data-click-id=\"body\" class=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\" href=\"/r/AskReddit/comments/cyqtk2/teachers_of_reddit_what_was_the_most_obvious/\"><div class=\"_2SdHzo12ISmrC8H86TgSCp _3wqmjmv3tb_k-PROt7qFZe \" style=\"--posttitletextcolor:#444e59\" theme=\"[object Object]\"><h3 class=\"_eYtD2XCVieq6emjKBH3m\">Teachers of Reddit, what was the most obvious \"teacher crush\" someone had on you?</h3></div></a>\n",
    "link = all_threads.find('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "referencia = link['href']\n",
    "texto_link = f'https://www.reddit.com{referencia}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8k\n",
      "Everyone has a scar on their body from something dumb, they did as a child. What's your story?\n",
      "https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/\n"
     ]
    }
   ],
   "source": [
    "# Resultado dos pontos e texto do primeiro top thread do dia:\n",
    "print(pontos_primeira_thread)\n",
    "print(texto_thread)\n",
    "print(texto_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo certo até aqui, então vamos seguir com os próximos passos.\n",
    "\n",
    "### Realizando um loop sobre todas as top threads desejadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "        \n",
    "print(len(extracted_points))\n",
    "print(len(extracted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse código, é possivel ver que a lista contendo as pontuações está duplicada. Provavelmente isto ocorre por haverem classes repitidas no código html. Para consertar isto, basta criarmos uma nova lista contendo os valores únicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28.8k', '28.8k', '3.3k', '3.3k', '2.7k', '2.7k', '1.2k', '1.2k', '1.5k', '1.5k', '812', '812', '936', '936', '563', '563']\n",
      "['28.8k', '3.3k', '2.7k', '1.2k', '1.5k', '812', '936', '563']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_points)\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "    \n",
    "print(unique_extracted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pontuacao': '28.8k', 'subreddit': '/r/AskReddit', 'titulo thread': \"Everyone has a scar on their body from something dumb, they did as a child. What's your story?\", 'link para os comentarios': 'https://www.reddit.com/r/AskReddit/comments/cz2apy/everyone_has_a_scar_on_their_body_from_something/'}]\n"
     ]
    }
   ],
   "source": [
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            \n",
    "print(top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opcionalmente podemos salvar o resultando em um arquivo json ou csv para uso futuro\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(top_threads, outfile, indent=4)\n",
    "    \n",
    "import csv\n",
    "csv_file = open('data.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['pontuacao', 'subreddit', 'thread', 'link'])\n",
    "\n",
    "# inserir csv_writer.writerow([p, s , t , l]) no loop das \"top_threads = []\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping na pagina principal\n",
    "\n",
    "Agora vamos ir para uma página mais geral do site www.reddit.com, e realizar um novo scrape (Novamente, a célula abaixo foi convertida em Raw NBConvert, para evitar rodá-la acidentalmente):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Salvando o html localmente:\n",
    "url = 'https://www.reddit.com/top/'\n",
    "codigo_html = requests.get(url)\n",
    "\n",
    "print(codigo_html.content[:1000])\n",
    "save_html(codigo_html.content, 'reddit_top_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pontuacao': '121k', 'subreddit': '/r/aww', 'titulo thread': 'Scared cat gets saved by two French guys', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyv97r/scared_cat_gets_saved_by_two_french_guys/'}\n",
      "\n",
      "{'pontuacao': '111k', 'subreddit': '/r/memes', 'titulo thread': 'The Area 51 raid is still happening right?', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cz2i20/the_area_51_raid_is_still_happening_right/'}\n",
      "\n",
      "{'pontuacao': '106k', 'subreddit': '/r/pics', 'titulo thread': 'In 1964, Ringo Starr snapped a photo of some high school students who skipped class to see the Beatles during their first trip to the US. The group had no idea the photo existed until Ringo published his book of photos. Nearly 50 years later, the group reunited and recreated the photo.', 'link para os comentarios': 'https://www.reddit.com/r/pics/comments/cyx1os/in_1964_ringo_starr_snapped_a_photo_of_some_high/'}\n",
      "\n",
      "{'pontuacao': '103k', 'subreddit': '/r/aww', 'titulo thread': 'Found a tiny danger noodle in need of assistance 🐍', 'link para os comentarios': 'https://www.reddit.com/r/aww/comments/cyx8xb/found_a_tiny_danger_noodle_in_need_of_assistance/'}\n",
      "\n",
      "{'pontuacao': '99.8k', 'subreddit': '/r/memes', 'titulo thread': 'I can feel it', 'link para os comentarios': 'https://www.reddit.com/r/memes/comments/cyu0z3/i_can_feel_it/'}\n",
      "\n",
      "{'pontuacao': '84.7k', 'subreddit': '/r/MurderedByWords', 'titulo thread': \"It's never just about a cake\", 'link para os comentarios': 'https://www.reddit.com/r/MurderedByWords/comments/cyt56i/its_never_just_about_a_cake/'}\n",
      "\n",
      "{'pontuacao': '80.1k', 'subreddit': '/r/mildlyinteresting', 'titulo thread': 'This vine climbed up a chair to silence my wind chime.', 'link para os comentarios': 'https://www.reddit.com/r/mildlyinteresting/comments/cyw9xx/this_vine_climbed_up_a_chair_to_silence_my_wind/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Código restante:\n",
    "\n",
    "html = open_html('reddit_top_today')\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "extracted_points = []\n",
    "for points in all_points_thread:\n",
    "    point = points.text\n",
    "    extracted_points.append(point)\n",
    "\n",
    "extracted_texts = []\n",
    "for threads in all_texts:\n",
    "    thread = threads.text\n",
    "    extracted_texts.append(thread)\n",
    "    \n",
    "extracted_links = []\n",
    "for links in all_links:\n",
    "    referencia = links['href']\n",
    "    if referencia.startswith('http'):\n",
    "        extracted_links.append(referencia)\n",
    "    else:\n",
    "        texto_link = f'https://www.reddit.com{referencia}'\n",
    "        extracted_links.append(texto_link)\n",
    "\n",
    "unique_extracted_points = []\n",
    "for i in range(0, len(extracted_points), 2):\n",
    "    unique_extracted_points.append(extracted_points[i])\n",
    "\n",
    "top_threads = []\n",
    "for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "    if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "        likes = p[0:-1]\n",
    "        likes = int(float(likes)*1000)\n",
    "        if likes >= 5000:\n",
    "            subreddit = l.split('/')[4]\n",
    "            s = f'/r/{subreddit}'\n",
    "            record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "            top_threads.append(record)\n",
    "            csv_writer.writerow([p, s , t , l])\n",
    "\n",
    "csv_file.close()\n",
    "for i in range(len(top_threads)):\n",
    "    print(top_threads[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o scrapping em uma lista de subreddits\n",
    "\n",
    "Agora que entendemos como realizar o scrappping em um único link, o próximo passo seria realizar o scraping a partir de uma lista de subreddits separados por ponto-e-vírgula, e.g., \"programming;dogs;brazil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "programming;dogs;brazil\n",
      "\n",
      " ['programming', 'dogs', 'brazil']\n"
     ]
    }
   ],
   "source": [
    "# input: programming;dogs;brazil\n",
    "subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "subreddits_separated = subreddits.split(';')\n",
    "\n",
    "print('\\n', subreddits_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/programming', 'https://www.reddit.com/r/dogs', 'https://www.reddit.com/r/brazil']\n"
     ]
    }
   ],
   "source": [
    "# A partir da lista criada anteriormente, cria-se outra lista com as urls para download do código html\n",
    "urls_subreddits = []\n",
    "for i in range(len(subreddits_separated)):\n",
    "    url_link = 'https://www.reddit.com/r/'+subreddits_separated[i]\n",
    "    urls_subreddits.append(url_link)\n",
    "\n",
    "print(urls_subreddits)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Aqui vamos salvar os htmls usando um for loop:\n",
    "\n",
    "for i in range(0, len(urls_subreddits)):\n",
    "    url = urls_subreddits[i]\n",
    "    codigo_html = requests.get(url)\n",
    "    save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "\n",
    "# html = open_html('reddit_brazil')\n",
    "# print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indo para a pasta principal, é possível ver que os arquivos foram salvos. Mas caso rodássemos o código acima, por conta da proteção que o site reddit possui, os htmls salvos não corresponderiam aos desejados. Por conta disto, foi necessário uma intervenção humana no looping para que fosse possível obter os htmls desejados, tendo que baixar um por vez.\n",
    "\n",
    "A partir da lista de htmls é possível realizar o mesmo scrapping anterior utilizando um loop do código anterior, como mostrado abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/programming:  []\n",
      "\n",
      "Top threads de /r/dogs:  []\n",
      "\n",
      "Top threads de /r/brazil:  []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    # Nesses dois casos (extracted_points e extracted_texts), é possível criarmos uma função, \n",
    "    # mas aqui como achei que o código não possui uma recorrência alta desses loops\n",
    "    # preferi deixá-los explícitos\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread) \n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "    \n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1 and p[-1] == 'k': # condição para evitar promoted threads e threads com menos de 1000 pontos\n",
    "            likes = p[0:-1]\n",
    "            likes = int(float(likes)*1000)\n",
    "            if likes >= 5000:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos casos acima, não houveram threads com mais de 5000 pontos. Além disto, foi possível rodar o código por conta da \"intervenção\" comentada anteriormente.\n",
    "\n",
    "Caso modificássemos o limite de 5000 pontos, para 500 pontos por exemplo, podemos obter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top threads de /r/programming:  [{'pontuacao': '2.1k', 'subreddit': '/r/programming', 'titulo thread': 'Former Google engineer breaks down interview problems he uses to screen candidates. Lots of good coding, algorithms, and interview tips.', 'link para os comentarios': 'https://www.reddit.com/r/programming/comments/cz6f5r/former_google_engineer_breaks_down_interview/'}]\n",
      "['2.1k', '438', '62', '77', '14', '12', '10', '16']\n",
      "\n",
      "Top threads de /r/dogs:  [{'pontuacao': '558', 'subreddit': '/r/dogs', 'titulo thread': '[RIP] Joey, beagle, 14 y.o.', 'link para os comentarios': 'https://www.reddit.com/r/dogs/comments/cz54pu/rip_joey_beagle_14_yo/'}, {'pontuacao': '999', 'subreddit': '/r/dogs', 'titulo thread': '[RIP] Tutya, pug, 4 y.o.', 'link para os comentarios': 'https://www.reddit.com/r/dogs/comments/cyst1s/rip_tutya_pug_4_yo/'}]\n",
      "['4', '9', '558', '85', '42', '16', '999', '24']\n",
      "\n",
      "Top threads de /r/brazil:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "['20', '24', '11', '4', '25', '65', '8', '44']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in subreddits_separated:\n",
    "    \n",
    "    html = open_html(f'reddit_{i}')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "    all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "    all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "    extracted_points = []\n",
    "    for points in all_points_thread:\n",
    "        point = points.text\n",
    "        extracted_points.append(point)\n",
    "\n",
    "    extracted_texts = []\n",
    "    for threads in all_texts:\n",
    "        thread = threads.text\n",
    "        extracted_texts.append(thread)\n",
    "\n",
    "    extracted_links = []\n",
    "    for links in all_links:\n",
    "        referencia = links['href']\n",
    "        if referencia.startswith('http'):\n",
    "            extracted_links.append(referencia)\n",
    "        else:\n",
    "            texto_link = f'https://www.reddit.com{referencia}'\n",
    "            extracted_links.append(texto_link)\n",
    "\n",
    "    unique_extracted_points = []\n",
    "    for j in range(0, len(extracted_points), 2):\n",
    "        unique_extracted_points.append(extracted_points[j])\n",
    "    \n",
    "    top_threads = []\n",
    "    for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "        if len(p) > 1: # Note que a partir daqui, o código foi modificado com o fim de obter as threads com pontuação menor que 1k\n",
    "            likes = p\n",
    "            if p[-1] == 'k':\n",
    "                likes = p[0:-1]\n",
    "                likes = int(float(likes)*1000)\n",
    "            else:\n",
    "                likes = int(likes)\n",
    "            if likes >= 500:\n",
    "                subreddit = l.split('/')[4]\n",
    "                s = f'/r/{subreddit}'\n",
    "                record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                top_threads.append(record)\n",
    "    if top_threads == []:\n",
    "        top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "    print(f'Top threads de /r/{i}: ', top_threads)\n",
    "    print(unique_extracted_points)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima, juntamente com os threads, também mandei imprimir os valores dos likes. Ao visitarmos as duas últimas  páginas ('r/dogs', 'r/brazil'), vemos que realmente não existem muitos pontos nos top diários. Assim, no caso em que não temos threads com o valor de pontos desejados, nosso código acaba não retornando valores/resultados para esses casos.\n",
    "\n",
    "## Resolução extra - Função para 'reddit scrapping':\n",
    "Por fim, podemos criar uma função que recebe os subreddits desejados e retorna as top threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_threads():\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    subreddits = str(input('Quais subreddits gostaria de acompanhar hoje? \\n'))\n",
    "    subreddits_separated = subreddits.split(';')\n",
    "    \n",
    "    urls_subreddits = []\n",
    "    for i in range(len(subreddits_separated)):\n",
    "        url_link = 'https://www.reddit.com/r/' + subreddits_separated[i]\n",
    "        urls_subreddits.append(url_link)\n",
    "    \n",
    "#     Aqui salvaríamos os htmls usando um for loop:\n",
    "#     for i in range(0, len(urls_subreddits)):\n",
    "#         url = urls_subreddits[i]\n",
    "#         codigo_html = requests.get(url)\n",
    "#         save_html(codigo_html.content, f'reddit_{subreddits_separated[i]}')\n",
    "\n",
    "    for i in subreddits_separated:\n",
    "\n",
    "        html = open_html(f'reddit_{i}')\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        table_threads = soup.find('div', class_=\"rpBJOHq2PR60pnwJlUyP0\")\n",
    "\n",
    "        all_points_thread = table_threads.find_all('div', class_=\"_1rZYMD_4xY3gRcSS3p8ODO\")\n",
    "        all_texts = table_threads.find_all('h3', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "        all_links = table_threads.find_all('a', class_=\"SQnoC3ObvgnGjWt90zD9Z _2INHSNB8V5eaWp4P0rY_mE\")\n",
    "\n",
    "        extracted_points = []\n",
    "        for points in all_points_thread:\n",
    "            point = points.text\n",
    "            extracted_points.append(point)\n",
    "\n",
    "        extracted_texts = []\n",
    "        for threads in all_texts:\n",
    "            thread = threads.text\n",
    "            extracted_texts.append(thread)\n",
    "\n",
    "        extracted_links = []\n",
    "        for links in all_links:\n",
    "            referencia = links['href']\n",
    "            if referencia.startswith('http'):\n",
    "                extracted_links.append(referencia)\n",
    "            else:\n",
    "                texto_link = f'https://www.reddit.com{referencia}'\n",
    "                extracted_links.append(texto_link)\n",
    "\n",
    "        unique_extracted_points = []\n",
    "        for j in range(0, len(extracted_points), 2):\n",
    "            unique_extracted_points.append(extracted_points[j])\n",
    "\n",
    "        top_threads = []\n",
    "        for p, t, l in zip(unique_extracted_points, extracted_texts, extracted_links):\n",
    "            if len(p) > 1: # Note que a partir daqui, o código foi modificado com o fim de obter as threads com pontuação menor que 1k\n",
    "                likes = p\n",
    "                if p[-1] == 'k':\n",
    "                    likes = p[0:-1]\n",
    "                    likes = int(float(likes)*1000)\n",
    "                else:\n",
    "                    likes = int(likes)\n",
    "                if likes >= 5000:\n",
    "                    subreddit = l.split('/')[4]\n",
    "                    s = f'/r/{subreddit}'\n",
    "                    record = {'pontuacao': p, 'subreddit': s, 'titulo thread': t, 'link para os comentarios': l}\n",
    "                    top_threads.append(record)\n",
    "        if top_threads == []:\n",
    "            top_threads = \"Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\"\n",
    "        print()\n",
    "        print(f'Top threads de /r/{i}: ', top_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quais subreddits gostaria de acompanhar hoje? \n",
      "programming;dogs;brazil\n",
      "\n",
      "Top threads de /r/programming:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/dogs:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n",
      "\n",
      "Top threads de /r/brazil:  Nao ha top threads com os requisitos desejados nesse dia. Volte amanha :)\n"
     ]
    }
   ],
   "source": [
    "# input: programming;dogs;brazil\n",
    "top_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
